{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from snorkel.labeling import LFAnalysis\n",
    "import seaborn as sns\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Distributions and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_correlation(df_signals):\n",
    "    coefs_df = np.zeros((len(df_signals.columns), len(df_signals.columns)))\n",
    "    for i, signal_i in enumerate(df_signals.columns):\n",
    "        for j, signal_j in enumerate(df_signals.columns):\n",
    "            corr = df_signals.loc[:, [signal_i, signal_j]]\n",
    "            corr = corr[corr != -1] # remove pairwise abstentions\n",
    "            coef = corr.corr().to_numpy()[0, 1]\n",
    "\n",
    "            coefs_df[i, j] = coef\n",
    "\n",
    "    df = pd.DataFrame(coefs_df, columns=df_signals.columns, index=df_signals.columns)\n",
    "    return df\n",
    "\n",
    "def plot_and_save_distribution(df_pos, df_neg, dataset_name):\n",
    "    objectives = ['ABSTAIN', 'Misinformation', 'Not Misinformation']\n",
    "    credibility_signals = df_pos.iloc[:, :19].columns\n",
    "    n_signals = 19\n",
    "    n_objectives = len(objectives)\n",
    "    bar_width = 0.8 / n_objectives\n",
    "    opacity = 0.7\n",
    "    pos = range(n_signals)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 14), sharex=True)\n",
    "    df = df_neg\n",
    "\n",
    "    # Initialize dictionaries to store percentages\n",
    "    percentages_0 = {}\n",
    "    percentages_1 = {}\n",
    "    percentages_minus_1 = {}\n",
    "\n",
    "    # Calculate the percentages of each value in each column\n",
    "    for col in df_pos.columns:\n",
    "        counts = df[col].value_counts(normalize=True)\n",
    "        percentages_0[col] = counts.get(0, 0) * 100\n",
    "        percentages_1[col] = counts.get(1, 0) * 100\n",
    "        percentages_minus_1[col] = counts.get(-1, 0) * 100\n",
    "\n",
    "    # Create a bar plot\n",
    "    width = 0.2  # Width of each bar\n",
    "    x = range(len(df.columns))\n",
    "    x_0 = [i - width for i in x]\n",
    "    x_1 = x\n",
    "    x_minus_1 = [i + width for i in x]\n",
    "\n",
    "    ax1.bar(x_0, percentages_0.values(), width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "    ax1.bar(x_1, percentages_1.values(), width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "    ax1.bar(x_minus_1, percentages_minus_1.values(), width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "\n",
    "    df = df_pos\n",
    "    # Initialize dictionaries to store percentages\n",
    "    percentages_0 = {}\n",
    "    percentages_1 = {}\n",
    "    percentages_minus_1 = {}\n",
    "\n",
    "    # Calculate the percentages of each value in each column\n",
    "    for col in df.columns:\n",
    "        counts = df[col].value_counts(normalize=True)\n",
    "        percentages_0[col] = counts.get(0, 0) * 100\n",
    "        percentages_1[col] = counts.get(1, 0) * 100\n",
    "        percentages_minus_1[col] = counts.get(-1, 0) * 100\n",
    "\n",
    "    # Create a bar plot\n",
    "    width = 0.2  # Width of each bar\n",
    "    x = range(len(df.columns))\n",
    "    x_0 = [i - width for i in x]\n",
    "    x_1 = x\n",
    "    x_minus_1 = [i + width for i in x]\n",
    "\n",
    "    ax2.bar(x_0, percentages_0.values(), width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "    ax2.bar(x_1, percentages_1.values(), width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "    ax2.bar(x_minus_1, percentages_minus_1.values(), width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "\n",
    "    # Set the x-axis labels\n",
    "    ax1.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "    ax1.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "    # ax1.set_ylabel('Average Percentage', fontsize=20)\n",
    "    ax1.set_title(\"Non-Misinformation Articles\", fontsize=25)\n",
    "\n",
    "    # Set the x-axis labels\n",
    "    ax2.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "    ax2.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "    ax2.set_xlabel('Credibility Signal', fontsize=20)\n",
    "    # ax2.set_ylabel('Mean', fontsize=20)\n",
    "    ax2.set_title(\"Misinformation Articles\", fontsize=25)\n",
    "\n",
    "    ax1.tick_params(axis='y', labelsize=20)\n",
    "    ax2.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "    # Create a single legend for both subplots\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    handles = handles1 + handles2\n",
    "    labels = labels1\n",
    "    legend = ax1.legend(handles, labels, title='Vote', fontsize=15, ncol=len(objectives), bbox_to_anchor=(0.5, 1.35), loc='upper center')\n",
    "    legend.get_title().set_fontsize(20)\n",
    "\n",
    "    def percent_formatter(x, pos):\n",
    "        return f\"{int(x)}%\"\n",
    "\n",
    "    ax1.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "    ax2.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"gossipcop\", \"fakenewsamt\", \"politifact\", \"celebrity\"]\n",
    "avg_correlations = []\n",
    "avg_analysis = []\n",
    "avg_corr_wrt_gt = []\n",
    "all_dfs = []\n",
    "all_neg = []\n",
    "all_pos = []\n",
    "\n",
    "signals = pd.read_csv(\"../data/signals.csv\").iloc[:,1].tolist()\n",
    "neg_percentages_0 = {k: [] for k in signals}\n",
    "neg_percentages_1 = {k: [] for k in signals}\n",
    "neg_percentages_minus_1 = {k: [] for k in signals}\n",
    "\n",
    "pos_percentages_0 = {k: [] for k in signals}\n",
    "pos_percentages_1 = {k: [] for k in signals}\n",
    "pos_percentages_minus_1 = {k: [] for k in signals}\n",
    "for dataset in datasets:\n",
    "    df_path = f\"../data/signals/{dataset}.csv\"\n",
    "    df = pd.read_csv(df_path)\n",
    "    df = df.drop(\"objective_pred\", axis=1)\n",
    "    df = df.rename({\"objective_true\": \"Ground Truth\"}, axis=1)\n",
    "\n",
    "    # print(dataset)\n",
    "    df_signals = df.iloc[:, :19]\n",
    "    y_true = df[\"Ground Truth\"].to_numpy()\n",
    "    correlation_df = get_pairwise_correlation(df.iloc[:, :20]) # Signals + ground truth\n",
    "\n",
    "\n",
    "    lf_analysis_df = LFAnalysis(df_signals.to_numpy()).lf_summary(y_true).set_index(df_signals.columns)\n",
    "    lf_analysis_df[\"Corr. wrt. GT\"] = correlation_df[\"Ground Truth\"]\n",
    "    # print(lf_analysis_df[\"Corr. wrt. GT\"])\n",
    "    avg_corr_wrt_gt.append(lf_analysis_df[\"Corr. wrt. GT\"].to_numpy())\n",
    "    avg_correlations.append(correlation_df)\n",
    "    avg_analysis.append(avg_analysis)\n",
    "    all_dfs.append(df)\n",
    "    \n",
    "    df_neg = df[df[\"Ground Truth\"] == 0].iloc[:, :19]\n",
    "    df_pos = df[df[\"Ground Truth\"] == 1].iloc[:, :19]\n",
    "\n",
    "    all_neg.append(df_neg)\n",
    "    all_pos.append(df_pos)\n",
    "    plot_and_save_distribution(df_neg=df_neg, df_pos=df_pos, dataset_name=dataset)\n",
    "    \n",
    "    for col in signals:\n",
    "        counts = df_neg[col].value_counts(normalize=True)\n",
    "        neg_percentages_0[col].append(counts.get(0, 0) * 100)\n",
    "        neg_percentages_1[col].append(counts.get(1, 0) * 100)\n",
    "        neg_percentages_minus_1[col].append(counts.get(-1, 0) * 100)\n",
    "\n",
    "        counts = df_pos[col].value_counts(normalize=True)\n",
    "        pos_percentages_0[col].append(counts.get(0, 0) * 100)\n",
    "        pos_percentages_1[col].append(counts.get(1, 0) * 100)\n",
    "        pos_percentages_minus_1[col].append(counts.get(-1, 0) * 100)\n",
    "\n",
    "for col in signals:\n",
    "    neg_percentages_0[col] = {\"mean\": np.mean(neg_percentages_0[col]), \"std\": np.std(neg_percentages_0[col])}\n",
    "    neg_percentages_1[col] = {\"mean\": np.mean(neg_percentages_1[col]), \"std\": np.std(neg_percentages_1[col])}\n",
    "    neg_percentages_minus_1[col] = {\"mean\": np.mean(neg_percentages_minus_1[col]), \"std\": np.std(neg_percentages_minus_1[col])}\n",
    "\n",
    "    pos_percentages_0[col] = {\"mean\": np.mean(pos_percentages_0[col]), \"std\": np.std(pos_percentages_0[col])}\n",
    "    pos_percentages_1[col] = {\"mean\": np.mean(pos_percentages_1[col]), \"std\": np.std(pos_percentages_1[col])}\n",
    "    pos_percentages_minus_1[col] = {\"mean\": np.mean(pos_percentages_minus_1[col]), \"std\": np.std(pos_percentages_minus_1[col])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives = ['ABSTAIN', 'Misinformation', 'Not Misinformation']\n",
    "credibility_signals = df.iloc[:, :19].columns\n",
    "n_signals = 19\n",
    "n_objectives = len(objectives)\n",
    "bar_width = 0.8 / n_objectives\n",
    "opacity = 0.7\n",
    "pos = range(n_signals)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 14), sharex=True)\n",
    "df = df_pos\n",
    "\n",
    "# Create a bar plot\n",
    "width = 0.2  # Width of each bar\n",
    "x = range(len(df.columns))\n",
    "x_0 = [i - width for i in x]\n",
    "x_1 = x\n",
    "x_minus_1 = [i + width for i in x]\n",
    "\n",
    "neg_means_0 = [v[\"mean\"] for v in neg_percentages_0.values()]\n",
    "neg_means_1 = [v[\"mean\"] for v in neg_percentages_1.values()]\n",
    "neg_means_minus_1 = [v[\"mean\"] for v in neg_percentages_minus_1.values()]\n",
    "neg_stds_0 = [v[\"std\"] for v in neg_percentages_0.values()]\n",
    "neg_stds_1 = [v[\"std\"] for v in neg_percentages_1.values()]\n",
    "neg_stds_minus_1 = [v[\"std\"] for v in neg_percentages_minus_1.values()]\n",
    "\n",
    "ax1.bar(x_0, neg_means_0, width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "ax1.errorbar(x_0, neg_means_0, yerr=neg_stds_0, fmt='none', ecolor='black')\n",
    "\n",
    "ax1.bar(x_1, neg_means_1, width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "ax1.errorbar(x_1, neg_means_1, yerr=neg_stds_1, fmt='none', ecolor='black')\n",
    "\n",
    "ax1.bar(x_minus_1, neg_means_minus_1, width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "ax1.errorbar(x_minus_1, neg_means_minus_1, yerr=neg_stds_minus_1, fmt='none', ecolor='black')\n",
    "\n",
    "df = df_neg\n",
    "\n",
    "# Create a bar plot\n",
    "width = 0.2  # Width of each bar\n",
    "x = range(len(df.columns))\n",
    "x_0 = [i - width for i in x]\n",
    "x_1 = x\n",
    "x_minus_1 = [i + width for i in x]\n",
    "\n",
    "pos_means_0 = [v[\"mean\"] for v in pos_percentages_0.values()]\n",
    "pos_means_1 = [v[\"mean\"] for v in pos_percentages_1.values()]\n",
    "pos_means_minus_1 = [v[\"mean\"] for v in pos_percentages_minus_1.values()]\n",
    "pos_stds_0 = [v[\"std\"] for v in pos_percentages_0.values()]\n",
    "pos_stds_1 = [v[\"std\"] for v in pos_percentages_1.values()]\n",
    "pos_stds_minus_1 = [v[\"std\"] for v in pos_percentages_minus_1.values()]\n",
    "\n",
    "\n",
    "ax2.bar(x_0, pos_means_0, width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "ax2.errorbar(x_0, pos_means_0, yerr=pos_stds_0, fmt='none', ecolor='black')\n",
    "\n",
    "ax2.bar(x_1, pos_means_1, width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "ax2.errorbar(x_1, pos_means_1, yerr=pos_stds_1, fmt='none', ecolor='black')\n",
    "\n",
    "ax2.bar(x_minus_1, pos_means_minus_1, width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "ax2.errorbar(x_minus_1, pos_means_minus_1, yerr=pos_stds_minus_1, fmt='none', ecolor='black')\n",
    "\n",
    "# Set the x-axis labels\n",
    "ax1.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "ax1.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "# ax1.set_ylabel('Average Percentage', fontsize=20)\n",
    "ax1.set_title(\"Non-Misinformation Articles\", fontsize=25)\n",
    "\n",
    "# Set the x-axis labels\n",
    "ax2.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "ax2.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "ax2.set_xlabel('Credibility Signal', fontsize=20)\n",
    "# ax2.set_ylabel('Mean', fontsize=20)\n",
    "ax2.set_title(\"Misinformation Articles\", fontsize=25)\n",
    "\n",
    "ax1.tick_params(axis='y', labelsize=20)\n",
    "ax2.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "# Create a single legend for both subplots\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "handles = handles1 + handles2\n",
    "labels = labels1\n",
    "legend = ax1.legend(handles, labels, title='Vote', fontsize=15, ncol=len(objectives), bbox_to_anchor=(0.5, 1.35), loc='upper center')\n",
    "legend.get_title().set_fontsize(20)\n",
    "\n",
    "def percent_formatter(x, pos):\n",
    "    return f\"{int(x)}%\"\n",
    "\n",
    "ax1.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "ax2.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# Define objectives and data\n",
    "objectives = ['No', 'Yes', 'Unsure']\n",
    "credibility_signals = df.iloc[:, :19].columns\n",
    "# Change to Polarizing Language to British English\n",
    "credibility_signals = [signal.replace(\"Polarizing\", \"Polarising\") for signal in credibility_signals]\n",
    "\n",
    "n_signals = len(credibility_signals)\n",
    "\n",
    "# Combine data into a single structure\n",
    "misinformation_means = [pos_means_0, pos_means_1, pos_means_minus_1]\n",
    "non_misinformation_means = [neg_means_0, neg_means_1, neg_means_minus_1]\n",
    "\n",
    "# Plot parameters\n",
    "bar_width = 0.4\n",
    "index = np.arange(n_signals)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Plotting the \"Non-Misinformation\" stacked bars with a solid pattern\n",
    "ax.bar(index - bar_width/2, non_misinformation_means[0], bar_width, label='Non-Misinformation - No', alpha=0.7, color='red')\n",
    "ax.bar(index - bar_width/2, non_misinformation_means[1], bar_width, label='Non-Misinformation - Yes', alpha=0.7, color='green', bottom=non_misinformation_means[0])\n",
    "ax.bar(index - bar_width/2, non_misinformation_means[2], bar_width, label='Non-Misinformation - Unsure', alpha=0.7, color='grey', bottom=np.add(non_misinformation_means[0], non_misinformation_means[1]))\n",
    "\n",
    "# Plotting the \"Misinformation\" stacked bars with a dashed edge pattern\n",
    "ax.bar(index + bar_width/2, misinformation_means[0], bar_width, label='Misinformation - No', alpha=0.7, color='red', hatch='//')\n",
    "ax.bar(index + bar_width/2, misinformation_means[1], bar_width, label='Misinformation - Yes', alpha=0.7, color='green', hatch='//', bottom=misinformation_means[0])\n",
    "ax.bar(index + bar_width/2, misinformation_means[2], bar_width, label='Misinformation - Unsure', alpha=0.7, color='grey', hatch='//', bottom=np.add(misinformation_means[0], misinformation_means[1]))\n",
    "\n",
    "# Set x-ticks and labels\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "\n",
    "# Set titles and labels\n",
    "# ax.set_title(\"Comparison of Credibility Signals\", fontsize=25, pad=50)  # Add padding to title\n",
    "ax.set_xlabel('Credibility Signal', fontsize=20)\n",
    "ax.set_ylabel('Average Percentage', fontsize=20)\n",
    "ax.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "# Adjust layout to make space for the legend below the title\n",
    "# plt.subplots_adjust(top=1.8)\n",
    "\n",
    "# Simplified legend\n",
    "handles = [\n",
    "    mpatches.Patch( facecolor=\"red\", label='No', edgecolor='black'),\n",
    "    mpatches.Patch( facecolor=\"green\", label='Yes', edgecolor='black'),\n",
    "    mpatches.Patch( facecolor=\"grey\", label='Unsure', edgecolor='black'),\n",
    "    mpatches.Patch( facecolor=\"white\", label='Non-Misinformation', edgecolor='black'),\n",
    "    mpatches.Patch( facecolor=\"white\", hatch=r'\\\\\\\\', label='Misinformation', edgecolor='black'),\n",
    "]\n",
    "legend = ax.legend(handles=handles, title='Legend', fontsize=15, ncol=2, bbox_to_anchor=(0.5, 1.2), loc='upper center')\n",
    "legend.get_title().set_fontsize(20)\n",
    "\n",
    "# Format y-axis as percentages\n",
    "def percent_formatter(x, pos):\n",
    "    return f\"{int(x)}%\"\n",
    "\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"../outputs/avg_signal_distributions.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisquare_test(df_neg, df_pos):\n",
    "    # Replace -1 with 0 and sum along axis 0 to get counts for each signal\n",
    "    neg_count = df_neg.replace(-1, 0).to_numpy().sum(axis=0)\n",
    "    pos_count = df_pos.replace(-1, 0).to_numpy().sum(axis=0)\n",
    "\n",
    "    # Store results\n",
    "    results = []\n",
    "    for signal, observed_neg, observed_pos in zip(signals, neg_count, pos_count):\n",
    "        # Combine the observed counts for chi-square\n",
    "        observed = np.array([observed_neg, observed_pos])\n",
    "\n",
    "        # Perform chi-square test\n",
    "        chi2, p = chisquare(f_obs=observed)\n",
    "\n",
    "        # Store the results\n",
    "        results.append((signal, chi2, p))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for dataset, neg, pos in zip(datasets, all_neg, all_pos):\n",
    "    result = chisquare_test(neg, pos)\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    # build dataset with result\n",
    "    result = pd.DataFrame(result, columns=[\"Signal\", \"Chi2\", \"p-value\"])\n",
    "\n",
    "    # add column to indicate if p-value is less than 0.05\n",
    "    result[\"p < 0.05\"] = result[\"p-value\"] < 0.05\n",
    "\n",
    "    # format the p-values to the 2nd nearest decimal before the exponent\n",
    "    result[\"p-value\"] = result[\"p-value\"].apply(lambda x: f\"{x:.2e}\")\n",
    "\n",
    "    # format the chi-square statistic to the 2nd nearest decimal\n",
    "    result[\"Chi2\"] = result[\"Chi2\"].apply(np.round, args=(2,))\n",
    "    \n",
    "    # normalise the chi2 statistic to the range [0, 1]\n",
    "    result[\"Chi2\"] = result[\"Chi2\"] / result[\"Chi2\"].max()\n",
    "\n",
    "    result[\"dataset\"] = dataset\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "results = pd.concat(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[results[\"Signal\"] == \"Polarizing Language\", \"Signal\"] = \"Polarising Language\"  # Change to British English\n",
    "results.loc[results[\"dataset\"] == \"gossipcop\", \"dataset\"] = \"GossipCop\"\n",
    "results.loc[results[\"dataset\"] == \"fakenewsamt\", \"dataset\"] = \"FakeNewsAMT\"\n",
    "results.loc[results[\"dataset\"] == \"politifact\", \"dataset\"] = \"PolitiFact\"\n",
    "results.loc[results[\"dataset\"] == \"celebrity\", \"dataset\"] = \"Celebrity\"\n",
    "\n",
    "# Create the heatmap DataFrame\n",
    "chi2_heatmap = results.pivot(index=\"dataset\", columns=\"Signal\", values=\"Chi2\")\n",
    "\n",
    "# Compute the average row\n",
    "average_row = chi2_heatmap.mean()\n",
    "politics_avg = chi2_heatmap.loc[[\"FakeNewsAMT\", \"PolitiFact\"], :].mean()\n",
    "entertainment_avg = chi2_heatmap.loc[[\"Celebrity\", \"GossipCop\"], :].mean()\n",
    "\n",
    "# Add a blank row before the 'Average' row\n",
    "chi2_heatmap.loc[\" \"] = np.nan  # Add a blank row\n",
    "\n",
    "chi2_heatmap.loc[\"Politics\", :] = politics_avg\n",
    "chi2_heatmap.loc[\"Entertainment\", :] = entertainment_avg\n",
    "chi2_heatmap.loc[\"All\", :] = average_row\n",
    "\n",
    "# Extract the p-value DataFrame\n",
    "pvalues = results.pivot(index=\"dataset\", columns=\"Signal\", values=\"p < 0.05\")\n",
    "pvalues.loc[\" \"] = False  # Add a blank row\n",
    "pvalues.loc[\"Politics\", :] = pvalues.loc[[\"FakeNewsAMT\", \"PolitiFact\"]].all()\n",
    "pvalues.loc[\"Entertainment\", :] = pvalues.loc[[\"Celebrity\", \"GossipCop\"]].all()\n",
    "pvalues.loc[\"All\", :] = pvalues.loc[[\"Celebrity\", \"GossipCop\", \"FakeNewsAMT\", \"PolitiFact\"]].all()\n",
    "\n",
    "# Extract the 'Average' row and sort by its values\n",
    "sorted_indices = chi2_heatmap.loc[\"All\"].sort_values(ascending=False).index\n",
    "\n",
    "# Reorder the DataFrame based on the sorted indices\n",
    "chi2_heatmap = chi2_heatmap[sorted_indices]  # Sort columns\n",
    "chi2_heatmap = chi2_heatmap.loc[chi2_heatmap.index]  # Ensure row order is maintained\n",
    "pvalues = pvalues[sorted_indices]  # Align pvalues DataFrame with sorted chi2_heatmap\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(chi2_heatmap, square=True, cmap=\"Reds\", annot=True, fmt=\".2f\", cbar=False, cbar_kws={\"size\": 5}, annot_kws={\"va\": \"top\"})\n",
    "\n",
    "# Add stars for significant values\n",
    "for i in range(pvalues.shape[0]):\n",
    "    for j in range(pvalues.shape[1]):\n",
    "        if pvalues.iloc[i, j]:  # If the p-value is significant\n",
    "            ax.text(j + 0.5, i + 0.5, '*', ha='center', va='bottom', color='black', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Adjust y-ticks to include the blank row\n",
    "ax.set_yticks(np.arange(len(chi2_heatmap.index)) + 0.5)\n",
    "ax.set_yticklabels(chi2_heatmap.index, rotation=0)\n",
    "\n",
    "# Get the y-ticks and hide the one corresponding to the blank row\n",
    "yticks = ax.yaxis.get_major_ticks()\n",
    "yticks[-4].set_visible(False)  # Hides the tick corresponding to the blank row\n",
    "\n",
    "# Rotate the x-ticks\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Remove both axis labels\n",
    "plt.xlabel(None)\n",
    "plt.ylabel(None)\n",
    "\n",
    "# Adjust the y-axis to clearly show the blank space\n",
    "# plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"outputs/chi2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_sorted_by_chi2 = list(chi2_heatmap.columns)\n",
    "signals_sorted_by_chi2 = [signal.replace(\"Polarising Language\", \"Polarizing Language\") for signal in signals_sorted_by_chi2]\n",
    "signals_sorted_by_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def get_train_dev_test_fold(fold, dataset, model_size, model_name=\"llama2_platypus\", num_splits=10):\n",
    "    assert fold < num_splits\n",
    "    \n",
    "    dataset_path = f\"../data/signals/{dataset}.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "    for j, (train_idxs, test_idxs) in enumerate(skf.split(range(len(df)), y=df[\"objective_true\"].to_numpy())):\n",
    "        train_df, test_df = df.iloc[train_idxs], df.iloc[test_idxs]\n",
    "        print(len(train_df)/len(df), len(test_df)/len(df))\n",
    "\n",
    "        if fold == j:\n",
    "            return train_df, test_df\n",
    "\n",
    "\n",
    "def predict_majority(row):\n",
    "    if len(row) == 1:\n",
    "        return row[0] if row[0] != -1 else np.random.choice([0, 1])\n",
    "    else:\n",
    "        # If there is a tie, randomly choose a class, else return the majority class\n",
    "        counts = row.value_counts().to_dict()\n",
    "        # get key with highest value\n",
    "        if -1 in counts:\n",
    "            del counts[-1]\n",
    "        \n",
    "        if len(counts) == 0:\n",
    "            return np.random.choice([0, 1])\n",
    "        else:\n",
    "            return max(counts, key=counts.get)\n",
    "\n",
    "\n",
    "\n",
    "signals_sorted_by_chi2 = list(chi2_heatmap.columns)\n",
    "signals_sorted_by_chi2 = [signal.replace(\"Polarising Language\", \"Polarizing Language\") for signal in signals_sorted_by_chi2]\n",
    "signals_sorted_by_chi2.reverse()\n",
    "print(signals_sorted_by_chi2)\n",
    "\n",
    "all = []\n",
    "best_signals_per_dataset = {}\n",
    "for dataset in [\"politifact\", \"fakenewsamt\", \"celebrity\", \"gossipcop\"]:\n",
    "    df = pd.read_csv(f\"../data/signals/{dataset}.csv\")\n",
    "\n",
    "    # cross validation loop\n",
    "    sf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    fold = 0\n",
    "    for train_index, test_index in sf.split(df, df[\"objective_true\"]):\n",
    "        df_train, df_test = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "        scores_by_num_signals = []\n",
    "        \n",
    "        random.seed()\n",
    "        y_test_gold = df_test[\"objective_true\"].to_numpy()\n",
    "\n",
    "        for i in range(1,20):\n",
    "            selected_signals = signals_sorted_by_chi2[:i]\n",
    "            L_ws_train = df_train.loc[:, selected_signals].to_numpy()\n",
    "            L_ws_test = df_test.loc[:, selected_signals].to_numpy()\n",
    "\n",
    "            label_model = LabelModel(cardinality=2, device=\"cpu\", verbose=False)\n",
    "            if i < 3:  # snorkel does not allow less than 3 signals, so append two columns with abstentions\n",
    "                L_ws_train = np.concatenate([L_ws_train, np.zeros((len(L_ws_train), 3-i))-1], axis=1)\n",
    "                L_ws_test = np.concatenate([L_ws_test, np.zeros((len(L_ws_test), 3-i))-1], axis=1)\n",
    "\n",
    "            label_model.fit(L_ws_train, n_epochs=500, seed=SEED, progress_bar=False)\n",
    "            y_pred_ws = label_model.predict(L=L_ws_test, tie_break_policy=\"random\")\n",
    "            val_f1_macro = f1_score(y_test_gold, y_pred_ws, average='macro', zero_division=0)\n",
    "\n",
    "            d = {\"dataset\": dataset, \"fold\":fold, \"f1\": val_f1_macro, \"#signals\": i}\n",
    "            all.append(d)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "# calculate the mean and stf of the f1 scores for each dataset and number of signals\n",
    "df = pd.DataFrame(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_sorted_by_chi2.reverse()\n",
    "df_grouped = df.groupby([\"dataset\", \"#signals\"]).mean().reset_index()\n",
    "df_grouped[\"std\"] = df.groupby([\"dataset\", \"#signals\"]).std().reset_index()[\"f1\"]\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"politifact\", \"dataset\"] = \"PolitiFact\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"gossipcop\", \"dataset\"] = \"GossipCop\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"fakenewsamt\", \"dataset\"] = \"FakeNewsAMT\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"celebrity\", \"dataset\"] = \"Celebrity\"\n",
    "\n",
    "df_grouped = df_grouped[[\"dataset\", \"#signals\", \"f1\", \"std\"]]\n",
    "\n",
    "# Get unique datasets\n",
    "unique_datasets = list(df_grouped[\"dataset\"].unique())\n",
    "fontsize=15\n",
    "# Plot each dataset with error bars\n",
    "for dataset in unique_datasets:\n",
    "    subset = df_grouped[df_grouped[\"dataset\"] == dataset]\n",
    "    # ax.errorbar(subset[\"#signals\"], subset[\"mu\"], yerr=subset[\"std_err\"], label=dataset)\n",
    "    ax.plot(subset[\"#signals\"], subset[\"f1\"], label=dataset, linewidth=2)\n",
    "\n",
    "# ax.set_xlabel('Signals', fontsize=fontsize)\n",
    "ax.set_ylabel(\"F1 Macro\", fontsize=fontsize)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "# ax.set_xlim(3, max(df[\"#signals\"]))\n",
    "ax.set_xlim(max(df[\"#signals\"]), 1)\n",
    "# ax.set_xticks([3] + list(df[\"#signals\"]))\n",
    "ax.tick_params(axis='y', labelsize=fontsize)\n",
    "ax.tick_params(axis='x', labelsize=10)\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "plt.yticks(np.arange(0.0, 1.0, 0.1))\n",
    "\n",
    "\n",
    "# add the signal names to the x axis\n",
    "ax.set_xticks(range(1, 20))\n",
    "\n",
    "\n",
    "# fix polarizing language to polarising language\n",
    "signals_sorted_by_chi2 = [signal.replace(\"Polarizing Language\", \"Polarising Language\") for signal in signals_sorted_by_chi2]\n",
    "\n",
    "ax.set_xticklabels([f\"({i+1}) - {v}\" for i, v in enumerate(signals_sorted_by_chi2)], rotation=45, ha='right', fontsize=fontsize-5)\n",
    "\n",
    "\n",
    "\n",
    "# plt.xticks(rotation=90, ha='right')\n",
    "legend = ax.legend(fontsize=fontsize-5)\n",
    "legend.get_title().set_fontsize(fontsize) \n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"signal_ablation.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_sorted_by_chi2 = list(chi2_heatmap.columns)\n",
    "# signals_sorted_by_chi2 = [signal for signal in signals_sorted_by_chi2 if signal != \"Inference\"]\n",
    "signals_sorted_by_chi2.reverse()\n",
    "signals_sorted_by_chi2.append(\" \")\n",
    "# signals_sorted_by_chi2.insert(0, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby([\"dataset\", \"#signals\"]).mean().reset_index()\n",
    "df_grouped[\"std\"] = df.groupby([\"dataset\", \"#signals\"]).std().reset_index()[\"f1\"]\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"politifact\", \"dataset\"] = \"PolitiFact\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"gossipcop\", \"dataset\"] = \"GossipCop\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"fakenewsamt\", \"dataset\"] = \"FakeNewsAMT\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"celebrity\", \"dataset\"] = \"Celebrity\"\n",
    "\n",
    "df_grouped = df_grouped[[\"dataset\", \"#signals\", \"f1\", \"std\"]]\n",
    "\n",
    "# Get unique datasets\n",
    "unique_datasets = list(df_grouped[\"dataset\"].unique())\n",
    "fontsize = 15\n",
    "\n",
    "# Plot each dataset with error bars\n",
    "for dataset in unique_datasets:\n",
    "    subset = df_grouped[df_grouped[\"dataset\"] == dataset]\n",
    "    ax.plot(subset[\"#signals\"], subset[\"f1\"], label=dataset, linewidth=2, alpha=0.8)\n",
    "\n",
    "average_f1 = df_grouped.groupby(\"#signals\")[\"f1\"].mean()\n",
    "ax.plot(average_f1.index, average_f1.values, label='Mean', linewidth=2, linestyle='--', color='black', alpha=1)\n",
    "\n",
    "# Annotate the average line values\n",
    "# for i, (x, y) in enumerate(zip(average_f1.index, average_f1.values)):\n",
    "#     ax.text(x-0.3, y+0.03, f\"{y:.3f}\".lstrip('0'), color=\"black\", fontsize=fontsize-3, ha='center', va='bottom')\n",
    "\n",
    "ax.set_ylabel(\"F1 Macro\", fontsize=fontsize)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Reverse the X-axis\n",
    "ax.set_xlim(max(df[\"#signals\"]), 1)\n",
    "\n",
    "# Set X-ticks and labels\n",
    "ax.set_xticks(range(0, 20))\n",
    "\n",
    "# Reverse the order of signal names for labels\n",
    "signals_sorted_by_chi2 = [signal.replace(\"Polarizing Language\", \"Polarising Language\") for signal in signals_sorted_by_chi2]\n",
    "ax.set_xticklabels([f\"({i}) - {v}\" for i, v in enumerate(signals_sorted_by_chi2)], rotation=45, ha='right', fontsize=fontsize-5)\n",
    "\n",
    "ax.tick_params(axis='y', labelsize=fontsize)\n",
    "ax.tick_params(axis='x', labelsize=10)\n",
    "ax.set_ylim(0.3, 1.0)\n",
    "plt.yticks(np.arange(0.3, 1.0, 0.1))\n",
    "\n",
    "legend = ax.legend(fontsize=fontsize-5)\n",
    "legend.get_title().set_fontsize(fontsize)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"signal_ablation.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ablation = df_grouped.groupby(\"#signals\")[\"f1\"].mean()\n",
    "# add the amount of improvement from the previous signal, starts with zero\n",
    "df_ablation = df_ablation.reset_index()\n",
    "df_ablation = pd.concat([pd.DataFrame([[0, 0.0]], columns=[\"#signals\", \"f1\"]), df_ablation])\n",
    "df_ablation = df_ablation.reset_index(drop=True)\n",
    "# insert a row with #signals=0 and f1 = 0\n",
    "df_ablation[\"Credibility Signal Removed\"] = signals_sorted_by_chi2\n",
    "df_ablation = df_ablation.sort_index(ascending=False)\n",
    "df_ablation[\"Decrease\"] = df_ablation[\"f1\"].diff().fillna(0)\n",
    "df_ablation[\"Decrease\"] = df_ablation[\"Decrease\"] * 100\n",
    "df_ablation[\"Decrease\"] = df_ablation[\"Decrease\"].apply(lambda x: f\"{np.round(x,1):.1f}%\")\n",
    "df_ablation[\"Avg. F1-Macro\"] = df_ablation[\"f1\"].apply(lambda x: f\"{np.round(x,3):.3f}\")\n",
    "df_ablation[\"Iteration\"] = np.arange(0,20)\n",
    "df_ablation[[\"Iteration\", \"Credibility Signal Removed\", \"Avg. F1-Macro\", \"Decrease\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Inference\"\n",
    "b = \"Call to Action\"\n",
    "\n",
    "test = pd.concat(all_dfs)\n",
    "test = test[(test[a] == 1) | (test[b] == 1)]\n",
    "test[\"equal\"] = test[a] == test[b]\n",
    "test[\"equal\"].sum() / len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "signals_sorted_by_chi2.pop()\n",
    "for i, dataset in enumerate([\"PolitiFact\", \"FakeNewsAMT\", \"Celebrity\", \"GossipCop\"]):\n",
    "    df = df_grouped[df_grouped[\"dataset\"] == dataset]\n",
    "    df = df.reset_index()\n",
    "    df[\"Credibility Signal added\"] = signals_sorted_by_chi2\n",
    "    df[\"Improvement\"] = df[\"f1\"].diff().fillna(0)\n",
    "    df[\"Improvement\"] = df[\"Improvement\"] * 100\n",
    "    df[\"Improvement\"] = df[\"Improvement\"].apply(lambda x: f\"{np.round(x,1):.1f}%\")\n",
    "    df[dataset] = df[\"f1\"].apply(lambda x: f\"{np.round(x,3):.3f}\")\n",
    "    df[\"Iteration\"] = np.arange(1,20)\n",
    "\n",
    "    if i > 0:\n",
    "        df = df[[dataset, \"Improvement\"]]\n",
    "    else:\n",
    "        df = df[[\"Iteration\", \"Credibility Signal added\", dataset, \"Improvement\"]].set_index(\"Iteration\")\n",
    "\n",
    "    df_all = pd.concat([df_all, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def get_train_dev_test_fold(fold, dataset, model_size, model_name=\"llama2_platypus\", num_splits=10):\n",
    "    assert fold < num_splits\n",
    "    \n",
    "    dataset_path = f\"../data/signals/{dataset}.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "    for j, (train_idxs, test_idxs) in enumerate(skf.split(range(len(df)), y=df[\"objective_true\"].to_numpy())):\n",
    "        train_df, test_df = df.iloc[train_idxs], df.iloc[test_idxs]\n",
    "        print(len(train_df)/len(df), len(test_df)/len(df))\n",
    "\n",
    "        if fold == j:\n",
    "            return train_df, test_df\n",
    "\n",
    "\n",
    "def predict_majority(row):\n",
    "    if len(row) == 1:\n",
    "        return row[0] if row[0] != -1 else np.random.choice([0, 1])\n",
    "    else:\n",
    "        # If there is a tie, randomly choose a class, else return the majority class\n",
    "        counts = row.value_counts().to_dict()\n",
    "        # get key with highest value\n",
    "        if -1 in counts:\n",
    "            del counts[-1]\n",
    "        \n",
    "        if len(counts) == 0:\n",
    "            return np.random.choice([0, 1])\n",
    "        else:\n",
    "            return max(counts, key=counts.get)\n",
    "\n",
    "\n",
    "\n",
    "signals_sorted_by_chi2 = list(chi2_heatmap.columns)\n",
    "signals_sorted_by_chi2 = [signal.replace(\"Polarising Language\", \"Polarizing Language\") for signal in signals_sorted_by_chi2]\n",
    "signals_sorted_by_chi2.reverse()\n",
    "\n",
    "all = []\n",
    "best_signals_per_dataset = {}\n",
    "for dataset in [\"politifact\", \"fakenewsamt\", \"celebrity\", \"gossipcop\"]:\n",
    "    df = pd.read_csv(f\"../data/signals/{dataset}.csv\")\n",
    "\n",
    "    # cross validation loop\n",
    "    sf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    fold = 0\n",
    "    for train_index, test_index in sf.split(df, df[\"objective_true\"]):\n",
    "        df_train, df_test = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "        scores_by_num_signals = []\n",
    "        \n",
    "        random.seed()\n",
    "        y_test_gold = df_test[\"objective_true\"].to_numpy()\n",
    "\n",
    "         \n",
    "        selected_signals = [signal for signal in signals_sorted_by_chi2 if signal != \"Misleading about content\"]\n",
    "        L_ws_train = df_train.loc[:, selected_signals].to_numpy()\n",
    "        L_ws_test = df_test.loc[:, selected_signals].to_numpy()\n",
    "\n",
    "        label_model = LabelModel(cardinality=2, device=\"cpu\", verbose=False)\n",
    "        label_model.fit(L_ws_train, n_epochs=500, seed=SEED, progress_bar=False)\n",
    "        y_pred_ws = label_model.predict(L=L_ws_test, tie_break_policy=\"random\")\n",
    "        val_f1_macro = f1_score(y_test_gold, y_pred_ws, average='macro', zero_division=0)\n",
    "\n",
    "        d = {\"dataset\": dataset, \"fold\":fold, \"f1\": val_f1_macro, \"#signals\": i}\n",
    "        all.append(d)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "# calculate the mean and stf of the f1 scores for each dataset and number of signals\n",
    "df = pd.DataFrame(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def get_train_dev_test_fold(fold, dataset, model_size, model_name=\"llama2_platypus\", num_splits=10):\n",
    "    assert fold < num_splits\n",
    "    \n",
    "    dataset_path = f\"../data/signals/{dataset}.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "    for j, (train_idxs, test_idxs) in enumerate(skf.split(range(len(df)), y=df[\"objective_true\"].to_numpy())):\n",
    "        train_df, test_df = df.iloc[train_idxs], df.iloc[test_idxs]\n",
    "        print(len(train_df)/len(df), len(test_df)/len(df))\n",
    "\n",
    "        if fold == j:\n",
    "            return train_df, test_df\n",
    "\n",
    "\n",
    "def predict_majority(row):\n",
    "    if len(row) == 1:\n",
    "        return row[0] if row[0] != -1 else np.random.choice([0, 1])\n",
    "    else:\n",
    "        # If there is a tie, randomly choose a class, else return the majority class\n",
    "        counts = row.value_counts().to_dict()\n",
    "        # get key with highest value\n",
    "        if -1 in counts:\n",
    "            del counts[-1]\n",
    "        \n",
    "        if len(counts) == 0:\n",
    "            return np.random.choice([0, 1])\n",
    "        else:\n",
    "            return max(counts, key=counts.get)\n",
    "\n",
    "\n",
    "\n",
    "signals_sorted_by_chi2 = list(chi2_heatmap.columns)\n",
    "signals_sorted_by_chi2 = [signal.replace(\"Polarising Language\", \"Polarizing Language\") for signal in signals_sorted_by_chi2]\n",
    "signals_sorted_by_chi2.reverse()\n",
    "\n",
    "all = []\n",
    "best_signals_per_dataset = {}\n",
    "for dataset in [\"politifact\", \"fakenewsamt\", \"celebrity\", \"gossipcop\"]:\n",
    "    df = pd.read_csv(f\"../data/signals/{dataset}.csv\")\n",
    "\n",
    "    # cross validation loop\n",
    "    sf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    fold = 0\n",
    "    for train_index, test_index in sf.split(range(len(df)), df[\"objective_true\"]):\n",
    "        df_train, df_test = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "        scores_by_num_signals = []\n",
    "        \n",
    "        random.seed()\n",
    "        y_test_gold = df_test[\"objective_true\"].to_numpy()\n",
    "\n",
    "\n",
    "        # Train with all signals\n",
    "        L_ws_train = df_train.loc[:, signals_sorted_by_chi2].to_numpy()\n",
    "        L_ws_test = df_test.loc[:, signals_sorted_by_chi2].to_numpy()\n",
    "\n",
    "        label_model = LabelModel(cardinality=2, device=\"cpu\", verbose=False)\n",
    "        label_model.fit(L_ws_train, n_epochs=500, seed=SEED, progress_bar=False)\n",
    "        y_pred_ws = label_model.predict(L=L_ws_test, tie_break_policy=\"random\")\n",
    "        baseline = f1_score(y_test_gold, y_pred_ws, average='macro', zero_division=0)\n",
    "\n",
    "        for i in range(1,20):\n",
    "            selected_signals = [signal for j, signal in enumerate(signals_sorted_by_chi2) if j+1 != i]\n",
    "            print(\"Removed signal\", signals_sorted_by_chi2[i-1])\n",
    "            L_ws_train = df_train.loc[:, selected_signals].to_numpy()\n",
    "            L_ws_test = df_test.loc[:, selected_signals].to_numpy()\n",
    "\n",
    "            label_model = LabelModel(cardinality=2, device=\"cpu\", verbose=False)\n",
    "            label_model.fit(L_ws_train, n_epochs=500, seed=SEED, progress_bar=False)\n",
    "            y_pred_ws = label_model.predict(L=L_ws_test, tie_break_policy=\"random\")\n",
    "            val_f1_macro = f1_score(y_test_gold, y_pred_ws, average='macro', zero_division=0)\n",
    "\n",
    "            d = {\"dataset\": dataset, \"fold\":fold, \"f1\": val_f1_macro, \"baseline_f1\": baseline, \"signal removed\": signals_sorted_by_chi2[i-1]}\n",
    "            print(baseline-val_f1_macro)\n",
    "            all.append(d)\n",
    "\n",
    "        \n",
    "\n",
    "        fold += 1\n",
    "\n",
    "# calculate the mean and stf of the f1 scores for each dataset and number of signals\n",
    "df = pd.DataFrame(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby([\"dataset\", \"signal removed\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage decrease from the column \"f1\" with respect to the column \"baseline_f1\" indicate with \"-\" if there was a decrease and \"+\" if there was an increase\n",
    "\n",
    "df[\"change\"] = (df[\"f1\"] - df[\"baseline_f1\"]) / df[\"baseline_f1\"]\n",
    "df = df[[\"dataset\", \"signal removed\", \"f1\", \"baseline_f1\", \"change\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each signal removed, display the decrease for each dataset\n",
    "df = df.pivot(index=\"signal removed\", columns=\"dataset\", values=\"change\")\n",
    "df = df.reset_index()\n",
    "# df = df[[\"signal removed\", \"FakeNewsAMT\", \"GossipCop\", \"PolitiFact\", \"Celebrity\"]]\n",
    "# df[\"mean\"] = df.iloc[:,1:].mean(axis=1)\n",
    "df[\"mean\"] = df.iloc[:,1:].mean(axis=1)\n",
    "df[\"entertainment\"] = df[[\"celebrity\", \"gossipcop\"]].mean(axis=1)\n",
    "df[\"politics\"] = df[[\"fakenewsamt\", \"politifact\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"mean\", ascending=True)\n",
    "\n",
    "# format the values to 2 decimal places, multiply by 100 and add the percentage sign\n",
    "df[\"mean\"] = df[\"mean\"].apply(lambda x: f\"{np.round(x*100,1):.1f}%\")\n",
    "df[\"celebrity\"] = df[\"celebrity\"].apply(lambda x: f\"{np.round(x*100,1):.1f}%\")\n",
    "df[\"fakenewsamt\"] = df[\"fakenewsamt\"].apply(lambda x: f\"{np.round(x*100,1):.1f}%\")\n",
    "df[\"gossipcop\"] = df[\"gossipcop\"].apply(lambda x: f\"{np.round(x*100,1):.1f}%\")\n",
    "df[\"politifact\"] = df[\"politifact\"].apply(lambda x: f\"{np.round(x*100,1):.1f}%\")\n",
    "df[\"politics\"] = df[\"politics\"].apply(lambda x: f\"{np.round(x*100,1):.1f}%\")\n",
    "df[\"entertainment\"] = df[\"entertainment\"].apply(lambda x: f\"{np.round(x*100,1):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby([\"dataset\", \"#signals\"]).mean().reset_index()\n",
    "df_grouped[\"std\"] = df.groupby([\"dataset\", \"#signals\"]).std().reset_index()[\"f1\"]\n",
    "\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"politifact\", \"dataset\"] = \"PolitiFact\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"gossipcop\", \"dataset\"] = \"GossipCop\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"fakenewsamt\", \"dataset\"] = \"FakeNewsAMT\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"celebrity\", \"dataset\"] = \"Celebrity\"\n",
    "\n",
    "df_grouped = df_grouped[[\"dataset\", \"#signals\", \"f1\", \"baseline_f1\", \"std\"]]\n",
    "df_ablation = df_grouped.groupby(\"#signals\").agg({\"f1\": \"mean\", \"baseline_f1\": \"mean\"})\n",
    "df_ablation[\"% decrease\"] = ((df_ablation[\"f1\"] - df_ablation[\"baseline_f1\"]) / df_ablation[\"baseline_f1\"]) * 100\n",
    "df_ablation = df_ablation.reset_index(drop=True)\n",
    "df_ablation[\"Credibility Signal Removed\"] = signals_sorted_by_chi2\n",
    "df_ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby([\"dataset\", \"#signals\"]).mean().reset_index()\n",
    "df_grouped[\"std\"] = df.groupby([\"dataset\", \"#signals\"]).std().reset_index()[\"f1\"]\n",
    "\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"politifact\", \"dataset\"] = \"PolitiFact\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"gossipcop\", \"dataset\"] = \"GossipCop\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"fakenewsamt\", \"dataset\"] = \"FakeNewsAMT\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"celebrity\", \"dataset\"] = \"Celebrity\"\n",
    "\n",
    "df_grouped = df_grouped[[\"dataset\", \"#signals\", \"f1\", \"std\"]]\n",
    "df_ablation = df_grouped.groupby(\"#signals\")[\"f1\"].mean()\n",
    "# add the amount of improvement from the previous signal, starts with zero\n",
    "df_ablation = df_ablation.reset_index()\n",
    "# df_ablation = pd.concat([pd.DataFrame([[0, 0.0]], columns=[\"#signals\", \"f1\"]), df_ablation])\n",
    "# df_ablation = df_ablation.reset_index(drop=True)\n",
    "# insert a row with #signals=0 and f1 = 0\n",
    "df_ablation[\"Credibility Signal Removed\"] = signals_sorted_by_chi2\n",
    "df_ablation = df_ablation.sort_index(ascending=False)\n",
    "df_ablation[\"Decrease\"] = df_ablation[\"f1\"].diff().fillna(0)\n",
    "df_ablation[\"Decrease\"] = df_ablation[\"Decrease\"] * 100\n",
    "df_ablation[\"Decrease\"] = df_ablation[\"Decrease\"].apply(lambda x: f\"{np.round(x,1):.1f}%\")\n",
    "df_ablation[\"Avg. F1-Macro\"] = df_ablation[\"f1\"].apply(lambda x: f\"{np.round(x,3):.3f}\")\n",
    "df_ablation[\"Iteration\"] = np.arange(1,20)\n",
    "df_ablation[[\"Iteration\", \"Credibility Signal Removed\", \"Avg. F1-Macro\", \"Decrease\"]].set_index(\"Iteration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cred_signalsv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
