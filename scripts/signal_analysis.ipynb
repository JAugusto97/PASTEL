{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from snorkel.labeling import LFAnalysis\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dev_test_fold(fold, dataset, model_size, model_name=\"llama2_platypus\", num_splits=10):\n",
    "    assert fold < num_splits\n",
    "    \n",
    "    dataset_path = f\"../data/processed/{dataset}/{model_name}/{model_size}/{dataset}.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "    for j, (train_idxs, test_idxs) in enumerate(skf.split(range(len(df)), y=df[\"objective_true\"].to_numpy())):\n",
    "        train_df, test_df = df.iloc[train_idxs], df.iloc[test_idxs]\n",
    "        print(len(train_df)/len(df), len(test_df)/len(df))\n",
    "\n",
    "        if fold == j:\n",
    "            return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_majority(row):\n",
    "    if len(row) == 1:\n",
    "        return row[0] if row[0] != -1 else np.random.choice([0, 1])\n",
    "    else:\n",
    "        # If there is a tie, randomly choose a class, else return the majority class\n",
    "        counts = row.value_counts().to_dict()\n",
    "        # get key with highest value\n",
    "        if -1 in counts:\n",
    "            del counts[-1]\n",
    "        \n",
    "        if len(counts) == 0:\n",
    "            return np.random.choice([0, 1])\n",
    "        else:\n",
    "            return max(counts, key=counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_sorted_by_corr = ['Document Citation', 'Sensationalism', 'Misleading about content',\n",
    "       'Evidence', 'Expert Citation', 'Emotional Valence',\n",
    "       'Reported by Other Sources', 'Clickbait', 'Source Credibility', 'Bias',\n",
    "       'Explicitly Unverified Claims', 'Polarizing Language', 'Informal Tone',\n",
    "       'Incorrect Spelling', 'Incivility', 'Personal Perspective', 'Inference',\n",
    "       'Impoliteness', 'Call to Action']\n",
    "\n",
    "signals_sorted_by_corr.reverse()\n",
    "\n",
    "all = []\n",
    "best_signals_per_dataset = {}\n",
    "for dataset in [\"politifact\", \"fakenewsamt\", \"celebrity\", \"gossipcop\"]:\n",
    "    print(dataset)\n",
    "\n",
    "    df = pd.read_csv(f\"../data/signals/{dataset}.csv\")\n",
    "\n",
    "    # cross validation loop\n",
    "    sf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "    fold = 0\n",
    "    for train_index, test_index in sf.split(df, df[\"objective_true\"]):\n",
    "        df_train, df_test = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "        scores_by_num_signals = []\n",
    "        \n",
    "        random.seed()\n",
    "        y_test_gold = df_test[\"objective_true\"].to_numpy()\n",
    "\n",
    "        for i in range(1,20):\n",
    "            selected_signals = signals_sorted_by_corr[:i]\n",
    "            L_ws_train = df_train.loc[:, selected_signals].to_numpy()\n",
    "            L_ws_test = df_test.loc[:, selected_signals].to_numpy()\n",
    "\n",
    "            label_model = LabelModel(cardinality=2, device=\"cpu\", verbose=False)\n",
    "            if i < 3:  # snorkel does not allow less than 3 signals, so append two columns with abstentions\n",
    "                L_ws_train = np.concatenate([L_ws_train, np.zeros((len(L_ws_train), 3-i))-1], axis=1)\n",
    "                L_ws_test = np.concatenate([L_ws_test, np.zeros((len(L_ws_test), 3-i))-1], axis=1)\n",
    "\n",
    "            label_model.fit(L_ws_train, n_epochs=500, seed=SEED, progress_bar=False)\n",
    "            y_pred_ws = label_model.predict(L=L_ws_test, tie_break_policy=\"random\")\n",
    "            val_f1_macro = f1_score(y_test_gold, y_pred_ws, average='macro', zero_division=0)\n",
    "\n",
    "            d = {\"dataset\": dataset, \"fold\":fold, \"f1\": val_f1_macro, \"#signals\": i}\n",
    "            all.append(d)\n",
    "\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and stf of the f1 scores for each dataset and number of signals\n",
    "df = pd.DataFrame(all)\n",
    "df_grouped = df.groupby([\"dataset\", \"#signals\"]).mean().reset_index()\n",
    "df_grouped[\"std\"] = df.groupby([\"dataset\", \"#signals\"]).std().reset_index()[\"f1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"politifact\", \"dataset\"] = \"PolitiFact\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"gossipcop\", \"dataset\"] = \"GossipCop\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"fakenewsamt\", \"dataset\"] = \"FakeNewsAMT\"\n",
    "df_grouped.loc[df_grouped[\"dataset\"] == \"celebrity\", \"dataset\"] = \"Celebrity\"\n",
    "\n",
    "# Get unique datasets\n",
    "unique_datasets = df_grouped[\"dataset\"].unique()\n",
    "fontsize=20\n",
    "# Plot each dataset with error bars\n",
    "for dataset in unique_datasets:\n",
    "    subset = df_grouped[df_grouped[\"dataset\"] == dataset]\n",
    "    # ax.errorbar(subset[\"#signals\"], subset[\"mu\"], yerr=subset[\"std_err\"], label=dataset)\n",
    "    ax.plot(subset[\"#signals\"], subset[\"f1\"], label=dataset, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('# Signals', fontsize=fontsize)\n",
    "ax.set_ylabel(\"F1 Macro\", fontsize=fontsize)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xlim(3, max(df[\"#signals\"]))\n",
    "ax.set_xticks([3] + list(df[\"#signals\"]))\n",
    "ax.tick_params(axis='y', labelsize=fontsize)\n",
    "ax.tick_params(axis='x', labelsize=fontsize)\n",
    "ax.set_ylim(0.30, 1.0)\n",
    "plt.yticks(np.arange(0.30, 1.1, 0.1))\n",
    "legend = ax.legend(fontsize=fontsize-5)\n",
    "legend.get_title().set_fontsize(fontsize) \n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"signal_ablation_sorted_corr_all.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Distributions and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_correlation(df_signals):\n",
    "    coefs_df = np.zeros((len(df_signals.columns), len(df_signals.columns)))\n",
    "    for i, signal_i in enumerate(df_signals.columns):\n",
    "        for j, signal_j in enumerate(df_signals.columns):\n",
    "            corr = df_signals.loc[:, [signal_i, signal_j]]\n",
    "            corr = corr[corr != -1] # remove pairwise abstentions\n",
    "            coef = corr.corr().to_numpy()[0, 1]\n",
    "\n",
    "            coefs_df[i, j] = coef\n",
    "\n",
    "    df = pd.DataFrame(coefs_df, columns=df_signals.columns, index=df_signals.columns)\n",
    "    return df\n",
    "\n",
    "def plot_and_save_distribution(df_pos, df_neg, dataset_name):\n",
    "    objectives = ['ABSTAIN', 'Misinformation', 'Not Misinformation']\n",
    "    credibility_signals = df_pos.iloc[:, :19].columns\n",
    "    n_signals = 19\n",
    "    n_objectives = len(objectives)\n",
    "    bar_width = 0.8 / n_objectives\n",
    "    opacity = 0.7\n",
    "    pos = range(n_signals)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 14), sharex=True)\n",
    "    df = df_neg\n",
    "\n",
    "    # Initialize dictionaries to store percentages\n",
    "    percentages_0 = {}\n",
    "    percentages_1 = {}\n",
    "    percentages_minus_1 = {}\n",
    "\n",
    "    # Calculate the percentages of each value in each column\n",
    "    for col in df_pos.columns:\n",
    "        counts = df[col].value_counts(normalize=True)\n",
    "        percentages_0[col] = counts.get(0, 0) * 100\n",
    "        percentages_1[col] = counts.get(1, 0) * 100\n",
    "        percentages_minus_1[col] = counts.get(-1, 0) * 100\n",
    "\n",
    "    # Create a bar plot\n",
    "    width = 0.2  # Width of each bar\n",
    "    x = range(len(df.columns))\n",
    "    x_0 = [i - width for i in x]\n",
    "    x_1 = x\n",
    "    x_minus_1 = [i + width for i in x]\n",
    "\n",
    "    ax1.bar(x_0, percentages_0.values(), width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "    ax1.bar(x_1, percentages_1.values(), width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "    ax1.bar(x_minus_1, percentages_minus_1.values(), width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "\n",
    "    df = df_pos\n",
    "    # Initialize dictionaries to store percentages\n",
    "    percentages_0 = {}\n",
    "    percentages_1 = {}\n",
    "    percentages_minus_1 = {}\n",
    "\n",
    "    # Calculate the percentages of each value in each column\n",
    "    for col in df.columns:\n",
    "        counts = df[col].value_counts(normalize=True)\n",
    "        percentages_0[col] = counts.get(0, 0) * 100\n",
    "        percentages_1[col] = counts.get(1, 0) * 100\n",
    "        percentages_minus_1[col] = counts.get(-1, 0) * 100\n",
    "\n",
    "    # Create a bar plot\n",
    "    width = 0.2  # Width of each bar\n",
    "    x = range(len(df.columns))\n",
    "    x_0 = [i - width for i in x]\n",
    "    x_1 = x\n",
    "    x_minus_1 = [i + width for i in x]\n",
    "\n",
    "    ax2.bar(x_0, percentages_0.values(), width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "    ax2.bar(x_1, percentages_1.values(), width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "    ax2.bar(x_minus_1, percentages_minus_1.values(), width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "\n",
    "    # Set the x-axis labels\n",
    "    ax1.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "    ax1.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "    # ax1.set_ylabel('Average Percentage', fontsize=20)\n",
    "    ax1.set_title(\"Non-Misinformation Articles\", fontsize=25)\n",
    "\n",
    "    # Set the x-axis labels\n",
    "    ax2.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "    ax2.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "    ax2.set_xlabel('Credibility Signal', fontsize=20)\n",
    "    # ax2.set_ylabel('Mean', fontsize=20)\n",
    "    ax2.set_title(\"Misinformation Articles\", fontsize=25)\n",
    "\n",
    "    ax1.tick_params(axis='y', labelsize=20)\n",
    "    ax2.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "    # Create a single legend for both subplots\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    handles = handles1 + handles2\n",
    "    labels = labels1\n",
    "    legend = ax1.legend(handles, labels, title='Vote', fontsize=15, ncol=len(objectives), bbox_to_anchor=(0.5, 1.35), loc='upper center')\n",
    "    legend.get_title().set_fontsize(20)\n",
    "\n",
    "    def percent_formatter(x, pos):\n",
    "        return f\"{int(x)}%\"\n",
    "\n",
    "    ax1.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "    ax2.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"gossipcop\", \"fakenewsamt\", \"politifact\", \"celebrity\"]\n",
    "avg_correlations = []\n",
    "avg_analysis = []\n",
    "avg_corr_wrt_gt = []\n",
    "all_dfs = []\n",
    "\n",
    "signals = pd.read_csv(\"../data/signals.csv\").iloc[:,1].tolist()\n",
    "neg_percentages_0 = {k: [] for k in signals}\n",
    "neg_percentages_1 = {k: [] for k in signals}\n",
    "neg_percentages_minus_1 = {k: [] for k in signals}\n",
    "\n",
    "pos_percentages_0 = {k: [] for k in signals}\n",
    "pos_percentages_1 = {k: [] for k in signals}\n",
    "pos_percentages_minus_1 = {k: [] for k in signals}\n",
    "for dataset in datasets:\n",
    "    df_path = f\"../data/signals/{dataset}.csv\"\n",
    "    df = pd.read_csv(df_path)\n",
    "    df = df.drop(\"objective_pred\", axis=1)\n",
    "    df = df.rename({\"objective_true\": \"Ground Truth\"}, axis=1)\n",
    "\n",
    "    # print(dataset)\n",
    "    df_signals = df.iloc[:, :19]\n",
    "    y_true = df[\"Ground Truth\"].to_numpy()\n",
    "    correlation_df = get_pairwise_correlation(df.iloc[:, :20]) # Signals + ground truth\n",
    "\n",
    "\n",
    "    lf_analysis_df = LFAnalysis(df_signals.to_numpy()).lf_summary(y_true).set_index(df_signals.columns)\n",
    "    lf_analysis_df[\"Corr. wrt. GT\"] = correlation_df[\"Ground Truth\"]\n",
    "    # print(lf_analysis_df[\"Corr. wrt. GT\"])\n",
    "    avg_corr_wrt_gt.append(lf_analysis_df[\"Corr. wrt. GT\"].to_numpy())\n",
    "    avg_correlations.append(correlation_df)\n",
    "    avg_analysis.append(avg_analysis)\n",
    "    all_dfs.append(df)\n",
    "    \n",
    "    df_neg = df[df[\"Ground Truth\"] == 0].iloc[:, :19]\n",
    "    df_pos = df[df[\"Ground Truth\"] == 1].iloc[:, :19]\n",
    "\n",
    "    plot_and_save_distribution(df_neg=df_neg, df_pos=df_pos, dataset_name=dataset)\n",
    "    \n",
    "    for col in signals:\n",
    "        counts = df_neg[col].value_counts(normalize=True)\n",
    "        neg_percentages_0[col].append(counts.get(0, 0) * 100)\n",
    "        neg_percentages_1[col].append(counts.get(1, 0) * 100)\n",
    "        neg_percentages_minus_1[col].append(counts.get(-1, 0) * 100)\n",
    "\n",
    "        counts = df_pos[col].value_counts(normalize=True)\n",
    "        pos_percentages_0[col].append(counts.get(0, 0) * 100)\n",
    "        pos_percentages_1[col].append(counts.get(1, 0) * 100)\n",
    "        pos_percentages_minus_1[col].append(counts.get(-1, 0) * 100)\n",
    "\n",
    "for col in signals:\n",
    "    neg_percentages_0[col] = {\"mean\": np.mean(neg_percentages_0[col]), \"std\": np.std(neg_percentages_0[col])}\n",
    "    neg_percentages_1[col] = {\"mean\": np.mean(neg_percentages_1[col]), \"std\": np.std(neg_percentages_1[col])}\n",
    "    neg_percentages_minus_1[col] = {\"mean\": np.mean(neg_percentages_minus_1[col]), \"std\": np.std(neg_percentages_minus_1[col])}\n",
    "\n",
    "    pos_percentages_0[col] = {\"mean\": np.mean(pos_percentages_0[col]), \"std\": np.std(pos_percentages_0[col])}\n",
    "    pos_percentages_1[col] = {\"mean\": np.mean(pos_percentages_1[col]), \"std\": np.std(pos_percentages_1[col])}\n",
    "    pos_percentages_minus_1[col] = {\"mean\": np.mean(pos_percentages_minus_1[col]), \"std\": np.std(pos_percentages_minus_1[col])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives = ['ABSTAIN', 'Misinformation', 'Not Misinformation']\n",
    "credibility_signals = df.iloc[:, :19].columns\n",
    "n_signals = 19\n",
    "n_objectives = len(objectives)\n",
    "bar_width = 0.8 / n_objectives\n",
    "opacity = 0.7\n",
    "pos = range(n_signals)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 14), sharex=True)\n",
    "df = df_pos\n",
    "\n",
    "# Create a bar plot\n",
    "width = 0.2  # Width of each bar\n",
    "x = range(len(df.columns))\n",
    "x_0 = [i - width for i in x]\n",
    "x_1 = x\n",
    "x_minus_1 = [i + width for i in x]\n",
    "\n",
    "neg_means_0 = [v[\"mean\"] for v in neg_percentages_0.values()]\n",
    "neg_means_1 = [v[\"mean\"] for v in neg_percentages_1.values()]\n",
    "neg_means_minus_1 = [v[\"mean\"] for v in neg_percentages_minus_1.values()]\n",
    "neg_stds_0 = [v[\"std\"] for v in neg_percentages_0.values()]\n",
    "neg_stds_1 = [v[\"std\"] for v in neg_percentages_1.values()]\n",
    "neg_stds_minus_1 = [v[\"std\"] for v in neg_percentages_minus_1.values()]\n",
    "\n",
    "ax1.bar(x_0, neg_means_0, width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "ax1.errorbar(x_0, neg_means_0, yerr=neg_stds_0, fmt='none', ecolor='black')\n",
    "\n",
    "ax1.bar(x_1, neg_means_1, width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "ax1.errorbar(x_1, neg_means_1, yerr=neg_stds_1, fmt='none', ecolor='black')\n",
    "\n",
    "ax1.bar(x_minus_1, neg_means_minus_1, width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "ax1.errorbar(x_minus_1, neg_means_minus_1, yerr=neg_stds_minus_1, fmt='none', ecolor='black')\n",
    "\n",
    "df = df_neg\n",
    "\n",
    "# Create a bar plot\n",
    "width = 0.2  # Width of each bar\n",
    "x = range(len(df.columns))\n",
    "x_0 = [i - width for i in x]\n",
    "x_1 = x\n",
    "x_minus_1 = [i + width for i in x]\n",
    "\n",
    "pos_means_0 = [v[\"mean\"] for v in pos_percentages_0.values()]\n",
    "pos_means_1 = [v[\"mean\"] for v in pos_percentages_1.values()]\n",
    "pos_means_minus_1 = [v[\"mean\"] for v in pos_percentages_minus_1.values()]\n",
    "pos_stds_0 = [v[\"std\"] for v in pos_percentages_0.values()]\n",
    "pos_stds_1 = [v[\"std\"] for v in pos_percentages_1.values()]\n",
    "pos_stds_minus_1 = [v[\"std\"] for v in pos_percentages_minus_1.values()]\n",
    "\n",
    "\n",
    "ax2.bar(x_0, pos_means_0, width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "ax2.errorbar(x_0, pos_means_0, yerr=pos_stds_0, fmt='none', ecolor='black')\n",
    "\n",
    "ax2.bar(x_1, pos_means_1, width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "ax2.errorbar(x_1, pos_means_1, yerr=pos_stds_1, fmt='none', ecolor='black')\n",
    "\n",
    "ax2.bar(x_minus_1, pos_means_minus_1, width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "ax2.errorbar(x_minus_1, pos_means_minus_1, yerr=pos_stds_minus_1, fmt='none', ecolor='black')\n",
    "\n",
    "# Set the x-axis labels\n",
    "ax1.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "ax1.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "# ax1.set_ylabel('Average Percentage', fontsize=20)\n",
    "ax1.set_title(\"Non-Misinformation Articles\", fontsize=25)\n",
    "\n",
    "# Set the x-axis labels\n",
    "ax2.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "ax2.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "ax2.set_xlabel('Credibility Signal', fontsize=20)\n",
    "# ax2.set_ylabel('Mean', fontsize=20)\n",
    "ax2.set_title(\"Misinformation Articles\", fontsize=25)\n",
    "\n",
    "ax1.tick_params(axis='y', labelsize=20)\n",
    "ax2.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "# Create a single legend for both subplots\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "handles = handles1 + handles2\n",
    "labels = labels1\n",
    "legend = ax1.legend(handles, labels, title='Vote', fontsize=15, ncol=len(objectives), bbox_to_anchor=(0.5, 1.35), loc='upper center')\n",
    "legend.get_title().set_fontsize(20)\n",
    "\n",
    "def percent_formatter(x, pos):\n",
    "    return f\"{int(x)}%\"\n",
    "\n",
    "ax1.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "ax2.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "correlations_df = pd.DataFrame(data=avg_corr_wrt_gt, index=datasets, columns=df_signals.columns)\n",
    "correlations_df = correlations_df.rename(\n",
    "    {\n",
    "        \"celebrity\": \"Celebrity\",\n",
    "        \"fakenewsamt\": \"FakeNewsAMT\",\n",
    "        \"gossipcop\": \"GossipCop\",\n",
    "        \"politifact\": \"PolitiFact\"\n",
    "    }, axis=0)\n",
    "correlations_df.loc[\"Average\"] = correlations_df.mean(axis=0)\n",
    "correlation_df = correlations_df.sort_values(\"Average\", axis=1, ascending=False) # sort by avg.\n",
    "correlation_df = correlation_df.transpose() # make it a vertical plot\n",
    "correlation_df = correlation_df[[\"Average\", \"PolitiFact\", \"FakeNewsAMT\", \"Celebrity\", \"GossipCop\"]]\n",
    "sns.heatmap(correlation_df, square=True, cmap=\"Reds\", annot=True, fmt=\".2f\", cbar=False, cbar_kws={\"size\": 5})\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"outputs/correlations_per_dataset.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cred_signalsv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
