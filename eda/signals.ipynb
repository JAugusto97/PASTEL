{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from snorkel.labeling import LFAnalysis\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_correlation(df_signals):\n",
    "    coefs_df = np.zeros((len(df_signals.columns), len(df_signals.columns)))\n",
    "    for i, signal_i in enumerate(df_signals.columns):\n",
    "        for j, signal_j in enumerate(df_signals.columns):\n",
    "            corr = df_signals.loc[:, [signal_i, signal_j]]\n",
    "            corr = corr[corr != -1] # remove pairwise abstentions\n",
    "            coef = corr.corr().to_numpy()[0, 1]\n",
    "\n",
    "            coefs_df[i, j] = coef\n",
    "\n",
    "    df = pd.DataFrame(coefs_df, columns=df_signals.columns, index=df_signals.columns)\n",
    "    return df\n",
    "\n",
    "def plot_and_save_distribution(df_pos, df_neg, dataset_name):\n",
    "    objectives = ['ABSTAIN', 'Misinformation', 'Not Misinformation']\n",
    "    credibility_signals = df_pos.iloc[:, :19].columns\n",
    "    n_signals = 19\n",
    "    n_objectives = len(objectives)\n",
    "    bar_width = 0.8 / n_objectives\n",
    "    opacity = 0.7\n",
    "    pos = range(n_signals)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 14), sharex=True)\n",
    "    df = df_neg\n",
    "\n",
    "    # Initialize dictionaries to store percentages\n",
    "    percentages_0 = {}\n",
    "    percentages_1 = {}\n",
    "    percentages_minus_1 = {}\n",
    "\n",
    "    # Calculate the percentages of each value in each column\n",
    "    for col in df_pos.columns:\n",
    "        counts = df[col].value_counts(normalize=True)\n",
    "        percentages_0[col] = counts.get(0, 0) * 100\n",
    "        percentages_1[col] = counts.get(1, 0) * 100\n",
    "        percentages_minus_1[col] = counts.get(-1, 0) * 100\n",
    "\n",
    "    # Create a bar plot\n",
    "    width = 0.2  # Width of each bar\n",
    "    x = range(len(df.columns))\n",
    "    x_0 = [i - width for i in x]\n",
    "    x_1 = x\n",
    "    x_minus_1 = [i + width for i in x]\n",
    "\n",
    "    ax1.bar(x_0, percentages_0.values(), width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "    ax1.bar(x_1, percentages_1.values(), width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "    ax1.bar(x_minus_1, percentages_minus_1.values(), width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "\n",
    "    df = df_pos\n",
    "    # Initialize dictionaries to store percentages\n",
    "    percentages_0 = {}\n",
    "    percentages_1 = {}\n",
    "    percentages_minus_1 = {}\n",
    "\n",
    "    # Calculate the percentages of each value in each column\n",
    "    for col in df.columns:\n",
    "        counts = df[col].value_counts(normalize=True)\n",
    "        percentages_0[col] = counts.get(0, 0) * 100\n",
    "        percentages_1[col] = counts.get(1, 0) * 100\n",
    "        percentages_minus_1[col] = counts.get(-1, 0) * 100\n",
    "\n",
    "    # Create a bar plot\n",
    "    width = 0.2  # Width of each bar\n",
    "    x = range(len(df.columns))\n",
    "    x_0 = [i - width for i in x]\n",
    "    x_1 = x\n",
    "    x_minus_1 = [i + width for i in x]\n",
    "\n",
    "    ax2.bar(x_0, percentages_0.values(), width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "    ax2.bar(x_1, percentages_1.values(), width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "    ax2.bar(x_minus_1, percentages_minus_1.values(), width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "\n",
    "    # Set the x-axis labels\n",
    "    ax1.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "    ax1.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "    # ax1.set_ylabel('Average Percentage', fontsize=20)\n",
    "    ax1.set_title(\"Non-Misinformation Articles\", fontsize=25)\n",
    "\n",
    "    # Set the x-axis labels\n",
    "    ax2.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "    ax2.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "    ax2.set_xlabel('Credibility Signal', fontsize=20)\n",
    "    # ax2.set_ylabel('Mean', fontsize=20)\n",
    "    ax2.set_title(\"Misinformation Articles\", fontsize=25)\n",
    "\n",
    "    ax1.tick_params(axis='y', labelsize=20)\n",
    "    ax2.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "    # Create a single legend for both subplots\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    handles = handles1 + handles2\n",
    "    labels = labels1\n",
    "    legend = ax1.legend(handles, labels, title='Vote', fontsize=15, ncol=len(objectives), bbox_to_anchor=(0.5, 1.35), loc='upper center')\n",
    "    legend.get_title().set_fontsize(20)\n",
    "\n",
    "    def percent_formatter(x, pos):\n",
    "        return f\"{int(x)}%\"\n",
    "\n",
    "    ax1.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "    ax2.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"outputs/{dataset_name}_signal_distributions.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"gossipcop\", \"fakenewsdataset\", \"politifact\", \"celebritydataset\"]\n",
    "avg_correlations = []\n",
    "avg_analysis = []\n",
    "avg_corr_wrt_gt = []\n",
    "all_dfs = []\n",
    "\n",
    "signals = pd.read_csv(\"../data/signals.csv\").iloc[:,1].tolist()\n",
    "neg_percentages_0 = {k: [] for k in signals}\n",
    "neg_percentages_1 = {k: [] for k in signals}\n",
    "neg_percentages_minus_1 = {k: [] for k in signals}\n",
    "\n",
    "pos_percentages_0 = {k: [] for k in signals}\n",
    "pos_percentages_1 = {k: [] for k in signals}\n",
    "pos_percentages_minus_1 = {k: [] for k in signals}\n",
    "for dataset in datasets:\n",
    "    df_path = f\"../data/processed/{dataset}/llama2_platypus/70/{dataset}.csv\"\n",
    "    df = pd.read_csv(df_path)\n",
    "    df = df.drop(\"objective_pred\", axis=1)\n",
    "    df = df.rename({\"objective_true\": \"Ground Truth\"}, axis=1)\n",
    "\n",
    "    # print(dataset)\n",
    "    df_signals = df.iloc[:, :19]\n",
    "    y_true = df[\"Ground Truth\"].to_numpy()\n",
    "    correlation_df = get_pairwise_correlation(df.iloc[:, :20]) # Signals + ground truth\n",
    "\n",
    "\n",
    "    lf_analysis_df = LFAnalysis(df_signals.to_numpy()).lf_summary(y_true).set_index(df_signals.columns)\n",
    "    lf_analysis_df[\"Corr. wrt. GT\"] = correlation_df[\"Ground Truth\"]\n",
    "    # print(lf_analysis_df[\"Corr. wrt. GT\"])\n",
    "    avg_corr_wrt_gt.append(lf_analysis_df[\"Corr. wrt. GT\"].to_numpy())\n",
    "    avg_correlations.append(correlation_df)\n",
    "    avg_analysis.append(avg_analysis)\n",
    "    all_dfs.append(df)\n",
    "    \n",
    "    df_neg = df[df[\"Ground Truth\"] == 0].iloc[:, :19]\n",
    "    df_pos = df[df[\"Ground Truth\"] == 1].iloc[:, :19]\n",
    "\n",
    "    plot_and_save_distribution(df_neg=df_neg, df_pos=df_pos, dataset_name=dataset)\n",
    "    \n",
    "    for col in signals:\n",
    "        counts = df_neg[col].value_counts(normalize=True)\n",
    "        neg_percentages_0[col].append(counts.get(0, 0) * 100)\n",
    "        neg_percentages_1[col].append(counts.get(1, 0) * 100)\n",
    "        neg_percentages_minus_1[col].append(counts.get(-1, 0) * 100)\n",
    "\n",
    "        counts = df_pos[col].value_counts(normalize=True)\n",
    "        pos_percentages_0[col].append(counts.get(0, 0) * 100)\n",
    "        pos_percentages_1[col].append(counts.get(1, 0) * 100)\n",
    "        pos_percentages_minus_1[col].append(counts.get(-1, 0) * 100)\n",
    "\n",
    "for col in signals:\n",
    "    neg_percentages_0[col] = {\"mean\": np.mean(neg_percentages_0[col]), \"std\": np.std(neg_percentages_0[col])}\n",
    "    neg_percentages_1[col] = {\"mean\": np.mean(neg_percentages_1[col]), \"std\": np.std(neg_percentages_1[col])}\n",
    "    neg_percentages_minus_1[col] = {\"mean\": np.mean(neg_percentages_minus_1[col]), \"std\": np.std(neg_percentages_minus_1[col])}\n",
    "\n",
    "    pos_percentages_0[col] = {\"mean\": np.mean(pos_percentages_0[col]), \"std\": np.std(pos_percentages_0[col])}\n",
    "    pos_percentages_1[col] = {\"mean\": np.mean(pos_percentages_1[col]), \"std\": np.std(pos_percentages_1[col])}\n",
    "    pos_percentages_minus_1[col] = {\"mean\": np.mean(pos_percentages_minus_1[col]), \"std\": np.std(pos_percentages_minus_1[col])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives = ['ABSTAIN', 'Misinformation', 'Not Misinformation']\n",
    "credibility_signals = df.iloc[:, :19].columns\n",
    "n_signals = 19\n",
    "n_objectives = len(objectives)\n",
    "bar_width = 0.8 / n_objectives\n",
    "opacity = 0.7\n",
    "pos = range(n_signals)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 14), sharex=True)\n",
    "df = df_pos\n",
    "\n",
    "# Create a bar plot\n",
    "width = 0.2  # Width of each bar\n",
    "x = range(len(df.columns))\n",
    "x_0 = [i - width for i in x]\n",
    "x_1 = x\n",
    "x_minus_1 = [i + width for i in x]\n",
    "\n",
    "neg_means_0 = [v[\"mean\"] for v in neg_percentages_0.values()]\n",
    "neg_means_1 = [v[\"mean\"] for v in neg_percentages_1.values()]\n",
    "neg_means_minus_1 = [v[\"mean\"] for v in neg_percentages_minus_1.values()]\n",
    "neg_stds_0 = [v[\"std\"] for v in neg_percentages_0.values()]\n",
    "neg_stds_1 = [v[\"std\"] for v in neg_percentages_1.values()]\n",
    "neg_stds_minus_1 = [v[\"std\"] for v in neg_percentages_minus_1.values()]\n",
    "\n",
    "ax1.bar(x_0, neg_means_0, width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "ax1.errorbar(x_0, neg_means_0, yerr=neg_stds_0, fmt='none', ecolor='black')\n",
    "\n",
    "ax1.bar(x_1, neg_means_1, width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "ax1.errorbar(x_1, neg_means_1, yerr=neg_stds_1, fmt='none', ecolor='black')\n",
    "\n",
    "ax1.bar(x_minus_1, neg_means_minus_1, width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "ax1.errorbar(x_minus_1, neg_means_minus_1, yerr=neg_stds_minus_1, fmt='none', ecolor='black')\n",
    "\n",
    "df = df_neg\n",
    "\n",
    "# Create a bar plot\n",
    "width = 0.2  # Width of each bar\n",
    "x = range(len(df.columns))\n",
    "x_0 = [i - width for i in x]\n",
    "x_1 = x\n",
    "x_minus_1 = [i + width for i in x]\n",
    "\n",
    "pos_means_0 = [v[\"mean\"] for v in pos_percentages_0.values()]\n",
    "pos_means_1 = [v[\"mean\"] for v in pos_percentages_1.values()]\n",
    "pos_means_minus_1 = [v[\"mean\"] for v in pos_percentages_minus_1.values()]\n",
    "pos_stds_0 = [v[\"std\"] for v in pos_percentages_0.values()]\n",
    "pos_stds_1 = [v[\"std\"] for v in pos_percentages_1.values()]\n",
    "pos_stds_minus_1 = [v[\"std\"] for v in pos_percentages_minus_1.values()]\n",
    "\n",
    "\n",
    "ax2.bar(x_0, pos_means_0, width=width, label='No', align='center', color=\"green\", alpha=opacity)\n",
    "ax2.errorbar(x_0, pos_means_0, yerr=pos_stds_0, fmt='none', ecolor='black')\n",
    "\n",
    "ax2.bar(x_1, pos_means_1, width=width, label='Yes', align='center', color=\"red\", alpha=opacity)\n",
    "ax2.errorbar(x_1, pos_means_1, yerr=pos_stds_1, fmt='none', ecolor='black')\n",
    "\n",
    "ax2.bar(x_minus_1, pos_means_minus_1, width=width, label='Unsure', align='center', color=\"grey\", alpha=opacity)\n",
    "ax2.errorbar(x_minus_1, pos_means_minus_1, yerr=pos_stds_minus_1, fmt='none', ecolor='black')\n",
    "\n",
    "# Set the x-axis labels\n",
    "ax1.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "ax1.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "# ax1.set_ylabel('Average Percentage', fontsize=20)\n",
    "ax1.set_title(\"Non-Misinformation Articles\", fontsize=25)\n",
    "\n",
    "# Set the x-axis labels\n",
    "ax2.set_xticks([p + (n_objectives - 1) * bar_width / 2 for p in pos])\n",
    "ax2.set_xticklabels(credibility_signals, rotation=45, ha='right', fontsize=20)\n",
    "ax2.set_xlabel('Credibility Signal', fontsize=20)\n",
    "# ax2.set_ylabel('Mean', fontsize=20)\n",
    "ax2.set_title(\"Misinformation Articles\", fontsize=25)\n",
    "\n",
    "ax1.tick_params(axis='y', labelsize=20)\n",
    "ax2.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "# Create a single legend for both subplots\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "handles = handles1 + handles2\n",
    "labels = labels1\n",
    "legend = ax1.legend(handles, labels, title='Vote', fontsize=15, ncol=len(objectives), bbox_to_anchor=(0.5, 1.35), loc='upper center')\n",
    "legend.get_title().set_fontsize(20)\n",
    "\n",
    "def percent_formatter(x, pos):\n",
    "    return f\"{int(x)}%\"\n",
    "\n",
    "ax1.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "ax2.yaxis.set_major_formatter(FuncFormatter(percent_formatter))\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/avg_signal_distributions.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "correlations_df = pd.DataFrame(data=avg_corr_wrt_gt, index=datasets, columns=df_signals.columns)\n",
    "correlations_df = correlations_df.rename(\n",
    "    {\n",
    "        \"celebritydataset\": \"Celebrity\",\n",
    "        \"fakenewsdataset\": \"FakeNewsAMT\",\n",
    "        \"gossipcop\": \"GossipCop\",\n",
    "        \"politifact\": \"PolitiFact\"\n",
    "    }, axis=0)\n",
    "correlations_df.loc[\"Average\"] = correlations_df.mean(axis=0)\n",
    "correlation_df = correlations_df.sort_values(\"Average\", axis=1, ascending=False) # sort by avg.\n",
    "correlation_df = correlation_df.transpose() # make it a vertical plot\n",
    "correlation_df = correlation_df[[\"Average\", \"PolitiFact\", \"FakeNewsAMT\", \"Celebrity\", \"GossipCop\"]]\n",
    "sns.heatmap(correlation_df, square=True, cmap=\"Reds\", annot=True, fmt=\".2f\", cbar=False, cbar_kws={\"size\": 5})\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/correlations_per_dataset.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise pearson correlation between all signals + veracity. Averaged across all datasets\n",
    "sns.heatmap(pd.DataFrame(np.mean(avg_correlations, axis=0), index=avg_correlations[0].index, columns=avg_correlations[0].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = df[df[\"Ground Truth\"] == 0].iloc[:, :19]\n",
    "df_pos = df[df[\"Ground Truth\"] == 1].iloc[:, :19]\n",
    "\n",
    "signals = df_pos.columns\n",
    "neg_percentages_0 = {k: [] for k in signals}\n",
    "neg_percentages_1 = {k: [] for k in signals}\n",
    "neg_percentages_minus_1 = {k: [] for k in signals}\n",
    "\n",
    "pos_percentages_0 = {k: [] for k in signals}\n",
    "pos_percentages_1 = {k: [] for k in signals}\n",
    "pos_percentages_minus_1 = {k: [] for k in signals}\n",
    "\n",
    "for col in signals:\n",
    "    counts = df_neg[col].value_counts(normalize=True)\n",
    "    neg_percentages_0[col].append(counts.get(0, 0) * 100)\n",
    "    neg_percentages_1[col].append(counts.get(1, 0) * 100)\n",
    "    neg_percentages_minus_1[col].append(counts.get(-1, 0) * 100)\n",
    "\n",
    "    counts = df_pos[col].value_counts(normalize=True)\n",
    "    pos_percentages_0[col].append(counts.get(0, 0) * 100)\n",
    "    pos_percentages_1[col].append(counts.get(1, 0) * 100)\n",
    "    pos_percentages_minus_1[col].append(counts.get(-1, 0) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_percentages_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages_0_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cred_signalsv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
