{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import llama_chat_hf\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-PCIE-40GB'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/fakenewsnet.csv\")\n",
    "df = df[df[\"dataset\"] == \"politifact\"]\n",
    "signal_df = pd.read_csv(\"../data/signals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.96s/it]\n"
     ]
    }
   ],
   "source": [
    "model = llama_chat_hf(size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_context = \\\n",
    "    \"\"\"You are a helpful and unbiased news verification assistant. You will be provided with the title and the full body of text of a news article. Then, you will answer further questions related to the given article. Ensure that your answers are grounded in reality, truthful and reliable. {abstain_context}It is essential that you only answer objectively with one of the following options: {options}. Please do not answer with anything other than the option provided.\"\"\"\n",
    "\n",
    "prompt = \"\"\"{title}\\n{text}\\n\\n{question} ({options})\"\"\"\n",
    "abstain_context = \"You are only allowed to answer with 'Unsure' if you do not have enough information or context to provide a reliable answer.\"\n",
    "abstain_context = \"\"\n",
    "# fake_news_context = \"Fake news is false or inaccurate information, especially that which is deliberately intended to deceive.\" # Decreases performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_mapping(answer):\n",
    "    if answer.lower().startswith(\"no\"):\n",
    "        category = 0\n",
    "    elif answer.lower().startswith(\"yes\"):\n",
    "        category = 1\n",
    "    else:\n",
    "        category = -1\n",
    "\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/922 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc=78.57, F1=63.48, Total=14, Num_Yes=1, Num_No=13, Num_Abstain=0:   0%|          | 0/922 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt_formatted \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat(title\u001b[39m=\u001b[39marticle_row\u001b[39m.\u001b[39mtitle, text\u001b[39m=\u001b[39marticle_row\u001b[39m.\u001b[39mtext, question\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan this article be categorised as fake news?\u001b[39m\u001b[39m\"\u001b[39m, options\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYes/No\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     answer_zs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mprompt(prompt_formatted, system_context\u001b[39m=\u001b[39;49msystem_context_zs, allow_abstain\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     10\u001b[0m \u001b[39mexcept\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mOutOfMemoryError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     11\u001b[0m     \u001b[39mcontinue\u001b[39;00m \u001b[39m# stop processing this example\u001b[39;00m\n",
      "File \u001b[0;32m~/code/prompted_credibility/prompting/utils.py:86\u001b[0m, in \u001b[0;36mllama_chat_hf.prompt\u001b[0;34m(self, user_message, allow_abstain, system_context)\u001b[0m\n\u001b[1;32m     82\u001b[0m     system_context \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[INST] <<SYS>>\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msystem_context\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m<</SYS>>\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m prompt \u001b[39m=\u001b[39m system_context \u001b[39m+\u001b[39m user_message\u001b[39m.\u001b[39mstrip() \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m [/INST] \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 86\u001b[0m ans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_next_word_probs(prompt, allow_abstain)\n\u001b[1;32m     87\u001b[0m \u001b[39m# ans = self.model.generate(self.tokenizer.encode(prompt, return_tensors='pt').to(self.device), max_new_tokens=max_new_tokens)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m# ans = self.tokenizer.decode(ans[0])\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m# ans = ans.split(\"[/INST]\")[1].strip()\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m ans\n",
      "File \u001b[0;32m~/code/prompted_credibility/prompting/utils.py:44\u001b[0m, in \u001b[0;36mllama_chat_hf.get_next_word_probs\u001b[0;34m(self, prefix, allow_abstain)\u001b[0m\n\u001b[1;32m     42\u001b[0m     top_token_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margsort(probabilities, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m     \u001b[39m# _, top_token_vals = torch.topk(probabilities, k=50)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     sorted_top_tokens \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(x)\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m top_token_vals]\n\u001b[1;32m     46\u001b[0m no_idx \u001b[39m=\u001b[39m sorted_top_tokens\u001b[39m.\u001b[39mindex(\u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sorted_top_tokens \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39minf\n\u001b[1;32m     47\u001b[0m yes_idx \u001b[39m=\u001b[39m sorted_top_tokens\u001b[39m.\u001b[39mindex(\u001b[39m\"\u001b[39m\u001b[39myes\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39myes\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sorted_top_tokens \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39minf\n",
      "File \u001b[0;32m~/code/prompted_credibility/prompting/utils.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     top_token_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margsort(probabilities, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m     \u001b[39m# _, top_token_vals = torch.topk(probabilities, k=50)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     sorted_top_tokens \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mdecode(x)\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m top_token_vals]\n\u001b[1;32m     46\u001b[0m no_idx \u001b[39m=\u001b[39m sorted_top_tokens\u001b[39m.\u001b[39mindex(\u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sorted_top_tokens \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39minf\n\u001b[1;32m     47\u001b[0m yes_idx \u001b[39m=\u001b[39m sorted_top_tokens\u001b[39m.\u001b[39mindex(\u001b[39m\"\u001b[39m\u001b[39myes\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39myes\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sorted_top_tokens \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39minf\n",
      "File \u001b[0;32m~/miniconda3/envs/cred_signalsv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3527\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3528\u001b[0m \u001b[39mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m   3529\u001b[0m \u001b[39mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3545\u001b[0m \u001b[39m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m   3546\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3547\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m-> 3548\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m   3550\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode(\n\u001b[1;32m   3551\u001b[0m     token_ids\u001b[39m=\u001b[39mtoken_ids,\n\u001b[1;32m   3552\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3553\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3554\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3555\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cred_signalsv2/lib/python3.10/site-packages/transformers/utils/generic.py:210\u001b[0m, in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    209\u001b[0m \u001b[39melif\u001b[39;00m is_torch_tensor(obj):\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    211\u001b[0m \u001b[39melif\u001b[39;00m is_jax_tensor(obj):\n\u001b[1;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(obj)\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "with tqdm(total=len(df)) as pbar:\n",
    "    for i, article_row in enumerate(df.sample(frac=1).itertuples()):\n",
    "        # ZS Question\n",
    "        system_context_zs = system_context.format(options=\"Yes/No\", abstain_context=\"\")\n",
    "        prompt_formatted = prompt.format(title=article_row.title, text=article_row.text, question=\"Can this article be categorised as fake news?\", options=\"Yes/No\")\n",
    "        try:\n",
    "            answer_zs = model.prompt(prompt_formatted, system_context=system_context_zs, allow_abstain=False)\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            continue # stop processing this example\n",
    "        \n",
    "        category = category_mapping(answer_zs)\n",
    "        preds.append(category)\n",
    "        label_converter = lambda x: 0 if x == \"real\" else 1\n",
    "        trues.append(label_converter(article_row.objective))\n",
    "\n",
    "        acc = accuracy_score(trues, preds)\n",
    "        f1 = f1_score(trues, preds, average=\"macro\")\n",
    "        num_yes = len([x for x in preds if x == 1])\n",
    "        num_no = len([x for x in preds if x == 0])\n",
    "        num_abstain = len([x for x in preds if x == -1])\n",
    "        updated_description = f\"Acc={acc*100:.2f}, F1={f1*100:.2f}, Total={i}, Num_Yes={num_yes}, Num_No={num_no}, Num_Abstain={num_abstain}\"\n",
    "        pbar.set_description(updated_description)\n",
    "        \n",
    "        # print(answer_zs)\n",
    "        # WS Questions\n",
    "        # print(10*\"-\")\n",
    "        # print(\"Objective:\", article_row.objective)\n",
    "        # for j, question_row in enumerate(signal_df.itertuples()):\n",
    "        #     system_context_ws = system_context.format(options=\"Yes/Unsure/No\", abstain_context=abstain_context)\n",
    "        #     prompt_formatted_ws = prompt.format(title=article_row.title, text=article_row.text, question=question_row.Question, options=\"Yes/Unsure/No\")\n",
    "\n",
    "        #     try:\n",
    "        #         answer_ws = model.prompt(prompt_formatted_ws, system_context=system_context_ws, allow_abstain=True)\n",
    "        #     except torch.cuda.OutOfMemoryError as e:\n",
    "        #         break # stop processing this example and the next questions\n",
    "\n",
    "        #     category_ws = category_mapping(answer_ws)\n",
    "        #     print(question_row.Question, category_ws)\n",
    "        #     print(answer_ws)\n",
    "        if i > 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 100.00%\n",
      "Acc: 0.5280898876404494\n",
      "F1-macro: 0.4159375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.93      0.67        46\n",
      "           1       0.57      0.09      0.16        43\n",
      "\n",
      "    accuracy                           0.53        89\n",
      "   macro avg       0.55      0.51      0.42        89\n",
      "weighted avg       0.55      0.53      0.42        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coverage: {len([x for x in preds if x != -1])/len(preds)*100:.2f}%\")\n",
    "print(\"Acc:\", accuracy_score(trues, preds))\n",
    "print(\"F1-macro:\", f1_score(trues, preds, average=\"macro\"))\n",
    "print(classification_report(trues, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cred_signalsv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
