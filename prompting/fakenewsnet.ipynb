{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DEVICE_NUM=4\n",
    "MODEL_SIZE=70\n",
    "DATASET=\"fakenewsnet\"\n",
    "VERBOSE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "CACHE_PATH = f\"../data/caches/{MODEL_SIZE}/cache.jsonl\"\n",
    "DATASET_PATH = f\"../data/{DATASET}.csv\"\n",
    "SIGNALS_PATH = \"../data/signals.csv\"\n",
    "\n",
    "assert os.path.exists(DATASET_PATH)\n",
    "assert os.path.exists(SIGNALS_PATH)\n",
    "if not os.path.exists(CACHE_PATH):\n",
    "    os.makedirs(CACHE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import llama_chat_hf\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Device Name:\", torch.cuda.get_device_name(), \"Device Number:\", DEVICE_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "signal_df = pd.read_csv(SIGNALS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llama_chat_hf(size=MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_context = \\\n",
    "    \"\"\"You are a helpful and unbiased news verification assistant. You will be provided with the title and the full body of text of a news article. Then, you will answer further questions related to the given article. Ensure that your answers are grounded in reality, truthful and reliable. {abstain_context}It is essential that you only answer objectively with one of the following options: {options}. Please do not answer with anything other than the option provided.\"\"\"\n",
    "\n",
    "prompt = \"\"\"{title}\\n{text}\\n\\n{question} ({options})\"\"\"\n",
    "# abstain_context = \"You are only allowed to answer with 'Unsure' if you do not have enough information or context to provide a reliable answer.\"\n",
    "abstain_context = \"\"\n",
    "# fake_news_context = \"Fake news is false or inaccurate information, especially that which is deliberately intended to deceive.\" # Decreases performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_mapping(answer):\n",
    "    if answer.lower().startswith(\"no\"):\n",
    "        category = 0\n",
    "    elif answer.lower().startswith(\"yes\"):\n",
    "        category = 1\n",
    "    else:\n",
    "        category = -1\n",
    "\n",
    "    return category\n",
    "\n",
    "def load_cache(p):\n",
    "    cache = []\n",
    "    if os.path.exists(p):\n",
    "        with open(p, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                cache.append(json.loads(line))\n",
    "            \n",
    "    return cache\n",
    "\n",
    "def dump_cache(line, p):\n",
    "    with open(p, \"a\") as f:\n",
    "        f.write(json.dumps(line)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(verbose=False):\n",
    "    preds = []\n",
    "    trues = []\n",
    "    processed_records = []\n",
    "    with tqdm(total=len(df)*len(signal_df)) as pbar:\n",
    "        for i, article_row in enumerate(df.itertuples()):\n",
    "            if article_row.article_md5 in [row[\"article_md5\"] for row in load_cache(CACHE_PATH)]:\n",
    "                continue\n",
    "\n",
    "            # ZS Question\n",
    "            system_context_zs = system_context.format(options=\"Yes/No\", abstain_context=\"\")\n",
    "            prompt_formatted = prompt.format(title=article_row.title, text=article_row.text, system_context=system_context_zs, question=\"Does this article contain misinformation?\", options=\"Yes/No\")\n",
    "            try:\n",
    "                answer_zs = model.prompt(prompt_formatted, allow_abstain=False)\n",
    "            except torch.cuda.OutOfMemoryError as e:\n",
    "                continue # stop processing this example\n",
    "            \n",
    "            category_zs = category_mapping(answer_zs)\n",
    "            preds.append(category_zs)\n",
    "            true = article_row.objective\n",
    "            trues.append(true)\n",
    "\n",
    "            acc = accuracy_score(trues, preds)\n",
    "            f1 = f1_score(trues, preds, average=\"macro\")\n",
    "            num_yes = len([x for x in preds if x == 1])\n",
    "            num_no = len([x for x in preds if x == 0])\n",
    "            num_abstain = len([x for x in preds if x == -1])\n",
    "            updated_description = f\"Acc={acc*100:.2f}, F1={f1*100:.2f}, Total={i}, Num_Yes={num_yes}, Num_No={num_no}, Num_Abstain={num_abstain}\"\n",
    "            pbar.set_description(updated_description)\n",
    "            \n",
    "            if verbose:\n",
    "                print(answer_zs)\n",
    "                \n",
    "                print(10*\"-\")\n",
    "                print(\"Objective:\", article_row.objective)\n",
    "\n",
    "            processed = {}\n",
    "            for j, question_row in enumerate(signal_df.itertuples()):\n",
    "                system_context_ws = system_context.format(options=\"Yes/Unsure/No\", abstain_context=abstain_context)\n",
    "                prompt_formatted_ws = prompt.format(title=article_row.title, text=article_row.text, question=question_row.Question, options=\"Yes/Unsure/No\")\n",
    "\n",
    "                try:\n",
    "                    answer_ws = model.prompt(prompt_formatted_ws, system_context=system_context_ws, allow_abstain=True)\n",
    "                except torch.cuda.OutOfMemoryError as e:\n",
    "                    break # stop processing this example and the next questions\n",
    "\n",
    "                if verbose:\n",
    "                    print(question_row.Question, category_ws)\n",
    "                    print(answer_ws)\n",
    "                    \n",
    "                category_ws = category_mapping(answer_ws)\n",
    "                processed[question_row._2] = category_ws\n",
    "                pbar.update(1)\n",
    "            \n",
    "            processed[\"objective_pred\"] = category_zs\n",
    "            processed[\"objective_true\"] = true\n",
    "            processed[\"article_md5\"] = article_row.article_md5\n",
    "            dump_cache(processed, CACHE_PATH)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process(verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "processed_records = []\n",
    "with tqdm(total=len(df)*len(signal_df)) as pbar:\n",
    "    for i, article_row in enumerate(df.itertuples()):\n",
    "        if article_row.article_md5 in [row[\"article_md5\"] for row in load_cache(CACHE_PATH)]:\n",
    "            continue\n",
    "\n",
    "        # ZS Question\n",
    "        system_context_zs = system_context.format(options=\"Yes/No\", abstain_context=\"\")\n",
    "        prompt_formatted = prompt.format(title=article_row.title, text=article_row.text, system_context=system_context_zs, question=\"Does this article contain misinformation?\", options=\"Yes/No\")\n",
    "        try:\n",
    "            answer_zs = model.prompt(prompt_formatted, allow_abstain=False)\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            continue # stop processing this example\n",
    "        \n",
    "        category_zs = category_mapping(answer_zs)\n",
    "        preds.append(category_zs)\n",
    "        label_converter = lambda x: 0 if x == \"real\" else 1\n",
    "        true = label_converter(article_row.objective)\n",
    "        trues.append(true)\n",
    "\n",
    "        acc = accuracy_score(trues, preds)\n",
    "        f1 = f1_score(trues, preds, average=\"macro\")\n",
    "        num_yes = len([x for x in preds if x == 1])\n",
    "        num_no = len([x for x in preds if x == 0])\n",
    "        num_abstain = len([x for x in preds if x == -1])\n",
    "        updated_description = f\"Acc={acc*100:.2f}, F1={f1*100:.2f}, Total={i}, Num_Yes={num_yes}, Num_No={num_no}, Num_Abstain={num_abstain}\"\n",
    "        pbar.set_description(updated_description)\n",
    "        \n",
    "        print(answer_zs)\n",
    "        \n",
    "        print(10*\"-\")\n",
    "        print(\"Objective:\", article_row.objective)\n",
    "\n",
    "        processed = {}\n",
    "        for j, question_row in enumerate(signal_df.itertuples()):\n",
    "            system_context_ws = system_context.format(options=\"Yes/Unsure/No\", abstain_context=abstain_context)\n",
    "            prompt_formatted_ws = prompt.format(title=article_row.title, text=article_row.text, question=question_row.Question, options=\"Yes/Unsure/No\")\n",
    "\n",
    "            try:\n",
    "                answer_ws = model.prompt(prompt_formatted_ws, system_context=system_context_ws, allow_abstain=True)\n",
    "            except torch.cuda.OutOfMemoryError as e:\n",
    "                break # stop processing this example and the next questions\n",
    "\n",
    "            category_ws = category_mapping(answer_ws)\n",
    "            print(question_row.Question, category_ws)\n",
    "            print(answer_ws)\n",
    "            processed[question_row._2] = category_ws\n",
    "            pbar.update(1)\n",
    "        \n",
    "        processed[\"objective_pred\"] = category_zs\n",
    "        processed[\"objective_true\"] = true\n",
    "        processed[\"article_md5\"] = article_row.article_md5\n",
    "        dump_cache(processed, CACHE_PATH)\n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cred_signalsv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
