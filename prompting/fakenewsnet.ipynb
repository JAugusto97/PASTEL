{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import llama_chat_hf\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-PCIE-40GB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = 7\n",
    "CACHE_PATH = F\"../data/caches/{MODEL_SIZE}/cache.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/fakenewsnet.csv\")\n",
    "signal_df = pd.read_csv(\"../data/signals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "df[\"article_md5\"] = df.apply(lambda x: hashlib.md5(str(x[\"title\"]+x[\"text\"]).encode('utf-8')).hexdigest(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model = llama_chat_hf(size=MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_context = \\\n",
    "    \"\"\"You are a helpful and unbiased news verification assistant. You will be provided with the title and the full body of text of a news article. Then, you will answer further questions related to the given article. Ensure that your answers are grounded in reality, truthful and reliable. {abstain_context}It is essential that you only answer objectively with one of the following options: {options}. Please do not answer with anything other than the option provided.\"\"\"\n",
    "\n",
    "prompt = \"\"\"{title}\\n{text}\\n\\n{question} ({options})\"\"\"\n",
    "# abstain_context = \"You are only allowed to answer with 'Unsure' if you do not have enough information or context to provide a reliable answer.\"\n",
    "abstain_context = \"\"\n",
    "# fake_news_context = \"Fake news is false or inaccurate information, especially that which is deliberately intended to deceive.\" # Decreases performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_mapping(answer):\n",
    "    if answer.lower().startswith(\"no\"):\n",
    "        category = 0\n",
    "    elif answer.lower().startswith(\"yes\"):\n",
    "        category = 1\n",
    "    else:\n",
    "        category = -1\n",
    "\n",
    "    return category\n",
    "\n",
    "def load_cache(p):\n",
    "    cache = []\n",
    "    if os.path.exists(p):\n",
    "        with open(p, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                cache.append(json.loads(line))\n",
    "            \n",
    "\n",
    "    return cache\n",
    "\n",
    "def dump_cache(line, p):\n",
    "    with open(p, \"a\") as f:\n",
    "        f.write(json.dumps(line)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/422617 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc=62.50, F1=61.90, Total=7, Num_Yes=5, Num_No=3, Num_Abstain=0:   0%|          | 143/422617 [01:59<89:54:46,  1.31it/s] "
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "processed_records = []\n",
    "with tqdm(total=len(df)*len(signal_df)) as pbar:\n",
    "    for i, article_row in enumerate(df.sample(frac=1).itertuples()):\n",
    "        if article_row.article_md5 in [row[\"article_md5\"] for row in load_cache(CACHE_PATH)]:\n",
    "            continue\n",
    "\n",
    "        # ZS Question\n",
    "        system_context_zs = system_context.format(options=\"Yes/No\", abstain_context=\"\")\n",
    "        prompt_formatted = prompt.format(title=article_row.title, text=article_row.text, question=\"Can this article be categorised as fake news?\", options=\"Yes/No\")\n",
    "        try:\n",
    "            answer_zs = model.prompt(prompt_formatted, system_context=system_context_zs, allow_abstain=False)\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            continue # stop processing this example\n",
    "        \n",
    "        category_zs = category_mapping(answer_zs)\n",
    "        preds.append(category_zs)\n",
    "        label_converter = lambda x: 0 if x == \"real\" else 1\n",
    "        true = label_converter(article_row.objective)\n",
    "        trues.append(true)\n",
    "\n",
    "        acc = accuracy_score(trues, preds)\n",
    "        f1 = f1_score(trues, preds, average=\"macro\")\n",
    "        num_yes = len([x for x in preds if x == 1])\n",
    "        num_no = len([x for x in preds if x == 0])\n",
    "        num_abstain = len([x for x in preds if x == -1])\n",
    "        updated_description = f\"Acc={acc*100:.2f}, F1={f1*100:.2f}, Total={i}, Num_Yes={num_yes}, Num_No={num_no}, Num_Abstain={num_abstain}\"\n",
    "        pbar.set_description(updated_description)\n",
    "        \n",
    "        # print(answer_zs)\n",
    "        # WS Questions\n",
    "        # print(10*\"-\")\n",
    "        # print(\"Objective:\", article_row.objective)\n",
    "\n",
    "        processed = {}\n",
    "        for j, question_row in enumerate(signal_df.itertuples()):\n",
    "            system_context_ws = system_context.format(options=\"Yes/Unsure/No\", abstain_context=abstain_context)\n",
    "            prompt_formatted_ws = prompt.format(title=article_row.title, text=article_row.text, question=question_row.Question, options=\"Yes/Unsure/No\")\n",
    "\n",
    "            try:\n",
    "                answer_ws = model.prompt(prompt_formatted_ws, system_context=system_context_ws, allow_abstain=True)\n",
    "            except torch.cuda.OutOfMemoryError as e:\n",
    "                break # stop processing this example and the next questions\n",
    "\n",
    "            category_ws = category_mapping(answer_ws)\n",
    "            # print(question_row.Question, category_ws)\n",
    "            # print(answer_ws)\n",
    "            processed[question_row._2] = category_ws\n",
    "            pbar.update(1)\n",
    "        \n",
    "        processed[\"objective_pred\"] = category_zs\n",
    "        processed[\"objective_true\"] = true\n",
    "        processed[\"article_md5\"] = article_row.article_md5\n",
    "        dump_cache(processed, CACHE_PATH)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 100.00%\n",
      "Acc: 1.0\n",
      "F1-macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coverage: {len([x for x in preds if x != -1])/len(preds)*100:.2f}%\")\n",
    "print(\"Acc:\", accuracy_score(trues, preds))\n",
    "print(\"F1-macro:\", f1_score(trues, preds, average=\"macro\"))\n",
    "print(classification_report(trues, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cred_signalsv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
